{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69072097-9599-4922-ae1a-434b8bd06417",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_newspapers = '../../data/corpora/newspapers_test/'\n",
    "# newspaper_test = 'newspaper_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "229469b7-084b-4003-a5e3-b1047fd6204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import codecs\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fde4ef85-446f-4928-9288-99f7eee5f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_expressions = {\"initials\": r\"\\b([A-Z][.](\\s)?)+\", \"prefixes\": r\"(Mr|St|Mrs|Ms|Dr|Esq|Sec|Secretar)[.]\",\\\n",
    "                     \"addresses\": \"\", \"dates\": \"\", \"line_break\": r\"¬\\n\", \"space\": r\"/s\",\\\n",
    "                     \"dashes\": r\"[-]+\", \"quote_marks\": r\"(“|”)\", \\\n",
    "                     \"months_abrv\": r\"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[.](\\s*(\\d{1,2})(,|\\.)?)?(\\s*\\d+)?\",\\\n",
    "                     \"pennies\": r\"(\\d+[.]?\\s*)[d][.]\", \"months_and_years\": r\"\\d{1,2}[.]\\s*(\\d{4})\"}\n",
    "\n",
    "titles = [\"Mr.\", \"St.\", \"Mrs.\", \"Ms.\", \"Dr.\", \"Esq.\", \"Sec.\", \"Secretar.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93281673-05ca-42e4-9199-0be58d1c7396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_corpus_of_txts(path=path_to_newspapers):\n",
    "    list_of_filenames_and_dirty_texts = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with codecs.open(path + filename, 'r', encoding='utf-8', errors=\"ignore\") as raw_text:\n",
    "                dirty_text = raw_text.read()\n",
    "            list_of_filenames_and_dirty_texts.append((filename, dirty_text))\n",
    "    return list_of_filenames_and_dirty_texts\n",
    "\n",
    "\n",
    "# strip all accented characters:\n",
    "def strip_accents(text):\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    return str(text)\n",
    "\n",
    "def process_periods(text):\n",
    "    # no matchobj needed since this is only called in other processing functions\n",
    "    text = re.sub(r\"[.]\",\"<prd>\", text)\n",
    "    return text\n",
    "\n",
    "def process_periods_to_commas(matchobj):\n",
    "    text = matchobj.group(0)\n",
    "    text = re.sub(r\"[.]\", \",\", text)\n",
    "    return text\n",
    "\n",
    "# processing functions for regex calls in preprocess_text() function\n",
    "# process initials for regex, and return a format that we can identify\n",
    "def process_initials(matchobj):\n",
    "    text = matchobj.group(0)\n",
    "    text = process_periods(text)\n",
    "    text = re.sub(r\"\\s*\", \"\", text)\n",
    "    text = text + \" \"\n",
    "    return text\n",
    "\n",
    "def process_months_abrv(matchobj):\n",
    "    text = matchobj.group(0)\n",
    "    text = process_periods(text)\n",
    "    # text = \" <date>\"+text+\"<date> \"\n",
    "    return text\n",
    "\n",
    "def process_pennies(matchobj):\n",
    "    text = matchobj.group(0)\n",
    "    text = re.sub(r\"d[.]?\",\"pennies\", text)\n",
    "    text = process_periods(text)\n",
    "    return text\n",
    "\n",
    "# combine it together\n",
    "def preprocess_text(text):\n",
    "    # remove all the line breaks created by newspaper processor\n",
    "    text = re.sub(regex_expressions[\"line_break\"],\"\", text)\n",
    "    # marking initials:\n",
    "    text = re.sub(regex_expressions[\"initials\"], process_initials, text)\n",
    "    # process titles:\n",
    "    text = re.sub(regex_expressions[\"prefixes\"],\"\\\\1<prd>\", text, flags=re.IGNORECASE)\n",
    "    # process month abbreviations:\n",
    "    text = re.sub(regex_expressions[\"months_abrv\"], process_months_abrv, text, flags=re.IGNORECASE)\n",
    "    # process instances of months [period] year:\n",
    "    text = re.sub(regex_expressions[\"months_and_years\"], process_periods_to_commas, text)\n",
    "    # process instances of \"No.\"\n",
    "    text = re.sub(r\"(No|Nos)[.]\",\"number\", text, flags=re.IGNORECASE)\n",
    "    # strip all dashes:\n",
    "    text = re.sub(regex_expressions[\"dashes\"], \" \", text)\n",
    "    # transform all quotes to ' \" ':\n",
    "    text = re.sub(regex_expressions[\"quote_marks\"], '\"', text)\n",
    "    # strip all pennies \"XX d.\" in the text:\n",
    "    text = re.sub(regex_expressions[\"pennies\"], process_pennies, text)\n",
    "    # strip all accents from the text:\n",
    "    text = strip_accents(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa2aee5-8d01-4f23-b83e-7f0453b372a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_tokenized_sent(sent):\n",
    "    # removing newline notations\n",
    "    clean_sent = re.sub('\\n', ' ', sent)\n",
    "    clean_sent = re.sub('\\r', ' ', clean_sent)\n",
    "    # transforming multiple spaces to one space\n",
    "    clean_sent = re.sub('\\s+',' ', clean_sent)\n",
    "    split_sentence = clean_sent.split()\n",
    "    \n",
    "    # transform all the words that are completely uppercase to lowercase\n",
    "    for index, word in enumerate(split_sentence):\n",
    "        if (word.isupper()):\n",
    "            new_word = word.lower()\n",
    "            split_sentence[index] = new_word\n",
    "    clean_sent = \" \".join(split_sentence)\n",
    "    \n",
    "    # put back the periods:\n",
    "    clean_sent = re.sub(\"<prd>\", \".\", clean_sent)\n",
    "    # clean_sent = clean_sent.lower()\n",
    "    return clean_sent\n",
    "\n",
    "\n",
    "### Not needed if done in df\n",
    "def clean_tokenized_list(sent_list):\n",
    "    cleaned_tokenized_sentences = []\n",
    "    for sent in sent_list:\n",
    "        clean_set = clean_tokenized_sent(sent)\n",
    "        cleaned_tokenized_sentences.append(clean_set)\n",
    "    return cleaned_tokenized_sentences\n",
    "\n",
    "def process_dirty_texts_to_df(list_of_filenames_and_dirty_texts):\n",
    "    cleaned_texts = []\n",
    "    cleaned_corpus_as_dictionary = {}\n",
    "    for filename, dirty_text in list_of_filenames_and_dirty_texts:\n",
    "        preprocessed_text = preprocess_text(dirty_text)\n",
    "        tokenized_sentences = sent_tokenize(preprocessed_text)\n",
    "        cleaned_tokenized_sentences = clean_tokenized_list(tokenized_sentences)\n",
    "        relative_sentence_index = 0\n",
    "        for clean_tokenized_sentence in cleaned_tokenized_sentences:\n",
    "            tupled_files = (filename, clean_tokenized_sentence, relative_sentence_index)\n",
    "            cleaned_texts.append(tupled_files)\n",
    "            relative_sentence_index += 1\n",
    "    cleaned_corpus_as_dictionary['file_names'] = [x[0] for x in cleaned_texts]\n",
    "    cleaned_corpus_as_dictionary['sentences'] = [x[1] for x in cleaned_texts]\n",
    "    cleaned_corpus_as_dictionary['relative_sentence_index'] = [x[2] for x in cleaned_texts]\n",
    "    \n",
    "    df = pd.DataFrame(cleaned_corpus_as_dictionary)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbe18ba6-8f69-44d0-bd58-96a55f34eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS try:\n",
    "# first loading english language support\n",
    "\n",
    "# faster but less accurate model:\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# slower but more accurate model:\n",
    "# download it first\n",
    "# !python -m spacy download en_core_web_lg\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def get_pos_counts_from_tagged_sentence(analyzed_sent):\n",
    "    pos_counts = []\n",
    "    pos_counts_raw = analyzed_sent.count_by(spacy.attrs.IDS['POS'])\n",
    "    for pos, count in pos_counts_raw.items():\n",
    "        tag = analyzed_sent.vocab[pos].text\n",
    "        pos_count = (tag, count)\n",
    "        pos_counts.append(pos_count)\n",
    "    # return a list of pos_counts\n",
    "    return pos_counts\n",
    "\n",
    "def pos_tag_sentence(sent):\n",
    "    tagged_sentence = []\n",
    "    analyzed_sent = nlp(sent, disable = ['ner'])\n",
    "    # getting the complete tokenized sentence\n",
    "    for token in analyzed_sent:\n",
    "        tagged_word = (token, token.pos_)\n",
    "        tagged_sentence.append(tagged_word)\n",
    "    pos_counts = get_pos_counts_from_tagged_sentence(analyzed_sent)\n",
    "    # return a tuple of both\n",
    "    return (tagged_sentence, pos_counts)\n",
    "\n",
    "def pos_tag_list_of_sentences(list_of_cleaned_sentences):\n",
    "    pos_tagged_text = []\n",
    "    for sent in list_of_cleaned_sentences:\n",
    "        tagged_sent = pos_tag_sentence(sent)\n",
    "        pos_tagged_text.append(tagged_sent)\n",
    "    # returns a list of tuples\n",
    "    return pos_tagged_text\n",
    "\n",
    "# this function takes in a tuple of two lists, \n",
    "# tagged_sentence and pos_counts\n",
    "# and checks for sequential occurences of title + name\n",
    "# if it finds them, it edits the two lists,\n",
    "# assigning the title to the proper noun after it,\n",
    "# and adjusting the pos_counts to reflect the change\n",
    "def remove_double_propernouns_from_tagged_sentence(tagged_sentence_and_counts):\n",
    "    # breaking apart our tuple into two lists\n",
    "    tagged_sentence, pos_counts = tagged_sentence_and_counts\n",
    "    # setting up a list of indexes to remove\n",
    "    indexes_to_remove = []\n",
    "    for index, tagged_word in enumerate(tagged_sentence):\n",
    "        # try is necessary here since we change the value of the next index before reaching it\n",
    "        # because we change it from a token to a string, the \".text\" method no longer works on the new\n",
    "        # word. If we catch this attributeError, we just continue, since we know that that word has been edited\n",
    "        try:\n",
    "            # checks that the tag is PROPN and that the word itself is a title\n",
    "            if ((tagged_word[1] == \"PROPN\") and (tagged_word[0].text in titles)):\n",
    "                # checks that the following word is also a proper noun\n",
    "                if tagged_sentence[index + 1][1] == \"PROPN\":\n",
    "                    # if so, get the title, and create a new value for the proceeding proper noun\n",
    "                    # with the title + white space + name\n",
    "                    # then place it back at it's proper index with the new values\n",
    "                    # and keep the index where we are at in a list, to remove them after\n",
    "                    identified_title = tagged_word[0].text\n",
    "                    new_proper_noun = identified_title + \" \" + tagged_sentence[index + 1][0].text # the next word in list\n",
    "                    tagged_sentence[index + 1] = (new_proper_noun, \"PROPN\")\n",
    "                    indexes_to_remove.append(int(index))\n",
    "\n",
    "        except AttributeError:\n",
    "            # Means that we've already changed that title since it's no longer a spacy token type\n",
    "            # so we should continue the loop\n",
    "            continue\n",
    "            \n",
    "    # first check if anything was removed in the sentence:\n",
    "    if len(indexes_to_remove) > 0:\n",
    "        number_of_titles_joined_to_names = len(indexes_to_remove)\n",
    "        # if something was, we remove values from the PROPN count\n",
    "        # equal to the number of changes made to the sentence\n",
    "        for index, pos_count in enumerate(pos_counts):\n",
    "            if pos_count[0] == \"PROPN\":\n",
    "                new_value = pos_count[1] - number_of_titles_joined_to_names\n",
    "                new_tuple = (pos_count[0], new_value)\n",
    "                # place new value back into list at index with the new tuple\n",
    "                pos_counts[index] = new_tuple\n",
    "                break\n",
    "        # finally, we create a new list of the tagged words with only the indexes NOT in our \"to_remove\" list\n",
    "        tagged_sentence = [tagged_word for index, tagged_word in enumerate(tagged_sentence) if index not in indexes_to_remove]\n",
    "    # and we return the new values\n",
    "    return (tagged_sentence, pos_counts)\n",
    "\n",
    "def pos_tag_texts_from_df(df, sentences_column='sentences'):\n",
    "    df['tagged_sentences'] = ''\n",
    "    df['pos_counts'] = ''\n",
    "    for index, row in df.iterrows():\n",
    "        sentence = row[sentences_column]\n",
    "        tagged_sentence_and_counts = pos_tag_sentence(sentence)\n",
    "        # if index < 100:\n",
    "        tagged_sentence, pos_counts = remove_double_propernouns_from_tagged_sentence(tagged_sentence_and_counts)\n",
    "        df.at[index, 'tagged_sentences'] = tagged_sentence\n",
    "        df.at[index, 'pos_counts'] = pos_counts\n",
    "        # if index < 20:\n",
    "        #     print(type(tagged_sentence), type(pos_counts))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28cd51a-8a84-47bd-a01e-3473919198b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40ce0572-03fa-4e08-9fb7-356c2cc2a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get total POS counts from df once ready\n",
    "# def get_total_pos_counts(list_of_pos_counts):\n",
    "#     for index, pos_counts in enumerate(list_of_pos_counts):\n",
    "#         for tag, count in pos_counts:\n",
    "#             print(tag)\n",
    "#             print(count)\n",
    "#         if index >= 10:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3529571-868c-4a9d-9144-11dd5228e15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>sentences</th>\n",
       "      <th>relative_sentence_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1879-08-01.txt</td>\n",
       "      <td>afe F.E. write immediately.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1879-08-01.txt</td>\n",
       "      <td>In London de vue de are 1 himney top.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1879-08-01.txt</td>\n",
       "      <td>E.E.F. , alive, weil, and at Hythe.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1879-08-01.txt</td>\n",
       "      <td>a Happy New Year to you sil.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1879-08-01.txt</td>\n",
       "      <td>ames.</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_names                              sentences  \\\n",
       "0  1879-08-01.txt            afe F.E. write immediately.   \n",
       "1  1879-08-01.txt  In London de vue de are 1 himney top.   \n",
       "2  1879-08-01.txt    E.E.F. , alive, weil, and at Hythe.   \n",
       "3  1879-08-01.txt           a Happy New Year to you sil.   \n",
       "4  1879-08-01.txt                                  ames.   \n",
       "\n",
       "   relative_sentence_index  \n",
       "0                        0  \n",
       "1                        1  \n",
       "2                        2  \n",
       "3                        3  \n",
       "4                        4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating a dictionary of all the texts with keys as their filenames\n",
    "dirty_texts = input_corpus_of_txts(path=path_to_newspapers)\n",
    "# print(dirty_texts)\n",
    "df = process_dirty_texts_to_df(dirty_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eddd60-f118-4993-bcac-130ee46c34a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "797dc42f-bf80-4aa3-a2db-a61158b2d352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>sentences</th>\n",
       "      <th>relative_sentence_index</th>\n",
       "      <th>tagged_sentences</th>\n",
       "      <th>pos_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1879-08-01.txt</td>\n",
       "      <td>afe F.E. write immediately.</td>\n",
       "      <td>0</td>\n",
       "      <td>[(afe, PROPN), (F.E., PROPN), (write, VERB), (...</td>\n",
       "      <td>[(PROPN, 2), (VERB, 1), (ADV, 1), (PUNCT, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1879-08-01.txt</td>\n",
       "      <td>In London de vue de are 1 himney top.</td>\n",
       "      <td>1</td>\n",
       "      <td>[(In, ADP), (London, PROPN), (de, X), (vue, X)...</td>\n",
       "      <td>[(ADP, 1), (PROPN, 1), (X, 3), (AUX, 1), (NUM,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1879-08-01.txt</td>\n",
       "      <td>E.E.F. , alive, weil, and at Hythe.</td>\n",
       "      <td>2</td>\n",
       "      <td>[(E.E.F., PROPN), (,, PUNCT), (alive, ADJ), (,...</td>\n",
       "      <td>[(PROPN, 3), (PUNCT, 4), (ADJ, 1), (CCONJ, 1),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1879-08-01.txt</td>\n",
       "      <td>a Happy New Year to you sil.</td>\n",
       "      <td>3</td>\n",
       "      <td>[(a, DET), (Happy, PROPN), (New, PROPN), (Year...</td>\n",
       "      <td>[(DET, 1), (PROPN, 3), (ADP, 1), (PRON, 1), (N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1879-08-01.txt</td>\n",
       "      <td>ames.</td>\n",
       "      <td>4</td>\n",
       "      <td>[(ames, PROPN), (., PUNCT)]</td>\n",
       "      <td>[(PROPN, 1), (PUNCT, 1)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_names                              sentences  \\\n",
       "0  1879-08-01.txt            afe F.E. write immediately.   \n",
       "1  1879-08-01.txt  In London de vue de are 1 himney top.   \n",
       "2  1879-08-01.txt    E.E.F. , alive, weil, and at Hythe.   \n",
       "3  1879-08-01.txt           a Happy New Year to you sil.   \n",
       "4  1879-08-01.txt                                  ames.   \n",
       "\n",
       "   relative_sentence_index                                   tagged_sentences  \\\n",
       "0                        0  [(afe, PROPN), (F.E., PROPN), (write, VERB), (...   \n",
       "1                        1  [(In, ADP), (London, PROPN), (de, X), (vue, X)...   \n",
       "2                        2  [(E.E.F., PROPN), (,, PUNCT), (alive, ADJ), (,...   \n",
       "3                        3  [(a, DET), (Happy, PROPN), (New, PROPN), (Year...   \n",
       "4                        4                        [(ames, PROPN), (., PUNCT)]   \n",
       "\n",
       "                                          pos_counts  \n",
       "0      [(PROPN, 2), (VERB, 1), (ADV, 1), (PUNCT, 1)]  \n",
       "1  [(ADP, 1), (PROPN, 1), (X, 3), (AUX, 1), (NUM,...  \n",
       "2  [(PROPN, 3), (PUNCT, 4), (ADJ, 1), (CCONJ, 1),...  \n",
       "3  [(DET, 1), (PROPN, 3), (ADP, 1), (PRON, 1), (N...  \n",
       "4                           [(PROPN, 1), (PUNCT, 1)]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pos_tag_texts_from_df(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "860c4022-e2e7-474d-b87c-4d766fb23cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>sentences</th>\n",
       "      <th>relative_sentence_index</th>\n",
       "      <th>tagged_sentences</th>\n",
       "      <th>pos_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1879-08-01.txt</td>\n",
       "      <td>afe F.E. write immediately.</td>\n",
       "      <td>0</td>\n",
       "      <td>[(afe, PROPN), (F.E., PROPN), (write, VERB), (...</td>\n",
       "      <td>[(PROPN, 2), (VERB, 1), (ADV, 1), (PUNCT, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1879-08-01.txt</td>\n",
       "      <td>In London de vue de are 1 himney top.</td>\n",
       "      <td>1</td>\n",
       "      <td>[(In, ADP), (London, PROPN), (de, X), (vue, X)...</td>\n",
       "      <td>[(ADP, 1), (PROPN, 1), (X, 3), (AUX, 1), (NUM,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1879-08-01.txt</td>\n",
       "      <td>E.E.F. , alive, weil, and at Hythe.</td>\n",
       "      <td>2</td>\n",
       "      <td>[(E.E.F., PROPN), (,, PUNCT), (alive, ADJ), (,...</td>\n",
       "      <td>[(PROPN, 3), (PUNCT, 4), (ADJ, 1), (CCONJ, 1),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1879-08-01.txt</td>\n",
       "      <td>a Happy New Year to you sil.</td>\n",
       "      <td>3</td>\n",
       "      <td>[(a, DET), (Happy, PROPN), (New, PROPN), (Year...</td>\n",
       "      <td>[(DET, 1), (PROPN, 3), (ADP, 1), (PRON, 1), (N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1879-08-01.txt</td>\n",
       "      <td>ames.</td>\n",
       "      <td>4</td>\n",
       "      <td>[(ames, PROPN), (., PUNCT)]</td>\n",
       "      <td>[(PROPN, 1), (PUNCT, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7926</th>\n",
       "      <td>1860-05-24.txt</td>\n",
       "      <td>hree pounds revard.left, in the clock aby (thi...</td>\n",
       "      <td>2572</td>\n",
       "      <td>[(hree, NUM), (pounds, NOUN), (revard.left, AD...</td>\n",
       "      <td>[(NUM, 3), (NOUN, 11), (ADV, 2), (PUNCT, 16), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7927</th>\n",
       "      <td>1860-05-24.txt</td>\n",
       "      <td>lost, a spanish threet cent.</td>\n",
       "      <td>2573</td>\n",
       "      <td>[(lost, VERB), (,, PUNCT), (a, DET), (spanish,...</td>\n",
       "      <td>[(VERB, 1), (PUNCT, 2), (DET, 1), (ADJ, 1), (N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7928</th>\n",
       "      <td>1860-05-24.txt</td>\n",
       "      <td>bond for 170, since the dividend of June, 1559...</td>\n",
       "      <td>2574</td>\n",
       "      <td>[(bond, NOUN), (for, ADP), (170, NUM), (,, PUN...</td>\n",
       "      <td>[(NOUN, 2), (ADP, 2), (NUM, 2), (PUNCT, 4), (S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7929</th>\n",
       "      <td>1860-05-24.txt</td>\n",
       "      <td>a dividend coupon is attached to said bond, wh...</td>\n",
       "      <td>2575</td>\n",
       "      <td>[(a, DET), (dividend, NOUN), (coupon, NOUN), (...</td>\n",
       "      <td>[(DET, 1), (NOUN, 4), (AUX, 2), (VERB, 2), (AD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7930</th>\n",
       "      <td>1860-05-24.txt</td>\n",
       "      <td>The above reward will be paid by M. Silverton,...</td>\n",
       "      <td>2576</td>\n",
       "      <td>[(The, DET), (above, ADJ), (reward, NOUN), (wi...</td>\n",
       "      <td>[(DET, 1), (ADJ, 1), (NOUN, 2), (AUX, 2), (VER...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7931 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_names                                          sentences  \\\n",
       "0     1879-08-01.txt                        afe F.E. write immediately.   \n",
       "1     1879-08-01.txt              In London de vue de are 1 himney top.   \n",
       "2     1879-08-01.txt                E.E.F. , alive, weil, and at Hythe.   \n",
       "3     1879-08-01.txt                       a Happy New Year to you sil.   \n",
       "4     1879-08-01.txt                                              ames.   \n",
       "...              ...                                                ...   \n",
       "7926  1860-05-24.txt  hree pounds revard.left, in the clock aby (thi...   \n",
       "7927  1860-05-24.txt                       lost, a spanish threet cent.   \n",
       "7928  1860-05-24.txt  bond for 170, since the dividend of June, 1559...   \n",
       "7929  1860-05-24.txt  a dividend coupon is attached to said bond, wh...   \n",
       "7930  1860-05-24.txt  The above reward will be paid by M. Silverton,...   \n",
       "\n",
       "      relative_sentence_index  \\\n",
       "0                           0   \n",
       "1                           1   \n",
       "2                           2   \n",
       "3                           3   \n",
       "4                           4   \n",
       "...                       ...   \n",
       "7926                     2572   \n",
       "7927                     2573   \n",
       "7928                     2574   \n",
       "7929                     2575   \n",
       "7930                     2576   \n",
       "\n",
       "                                       tagged_sentences  \\\n",
       "0     [(afe, PROPN), (F.E., PROPN), (write, VERB), (...   \n",
       "1     [(In, ADP), (London, PROPN), (de, X), (vue, X)...   \n",
       "2     [(E.E.F., PROPN), (,, PUNCT), (alive, ADJ), (,...   \n",
       "3     [(a, DET), (Happy, PROPN), (New, PROPN), (Year...   \n",
       "4                           [(ames, PROPN), (., PUNCT)]   \n",
       "...                                                 ...   \n",
       "7926  [(hree, NUM), (pounds, NOUN), (revard.left, AD...   \n",
       "7927  [(lost, VERB), (,, PUNCT), (a, DET), (spanish,...   \n",
       "7928  [(bond, NOUN), (for, ADP), (170, NUM), (,, PUN...   \n",
       "7929  [(a, DET), (dividend, NOUN), (coupon, NOUN), (...   \n",
       "7930  [(The, DET), (above, ADJ), (reward, NOUN), (wi...   \n",
       "\n",
       "                                             pos_counts  \n",
       "0         [(PROPN, 2), (VERB, 1), (ADV, 1), (PUNCT, 1)]  \n",
       "1     [(ADP, 1), (PROPN, 1), (X, 3), (AUX, 1), (NUM,...  \n",
       "2     [(PROPN, 3), (PUNCT, 4), (ADJ, 1), (CCONJ, 1),...  \n",
       "3     [(DET, 1), (PROPN, 3), (ADP, 1), (PRON, 1), (N...  \n",
       "4                              [(PROPN, 1), (PUNCT, 1)]  \n",
       "...                                                 ...  \n",
       "7926  [(NUM, 3), (NOUN, 11), (ADV, 2), (PUNCT, 16), ...  \n",
       "7927  [(VERB, 1), (PUNCT, 2), (DET, 1), (ADJ, 1), (N...  \n",
       "7928  [(NOUN, 2), (ADP, 2), (NUM, 2), (PUNCT, 4), (S...  \n",
       "7929  [(DET, 1), (NOUN, 4), (AUX, 2), (VERB, 2), (AD...  \n",
       "7930  [(DET, 1), (ADJ, 1), (NOUN, 2), (AUX, 2), (VER...  \n",
       "\n",
       "[7931 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f88bc018-540f-471b-becc-c8488dc36319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(The, 'DET'),\n",
       " (above, 'ADJ'),\n",
       " (reward, 'NOUN'),\n",
       " (will, 'AUX'),\n",
       " (be, 'AUX'),\n",
       " (paid, 'VERB'),\n",
       " (by, 'ADP'),\n",
       " (M., 'PROPN'),\n",
       " (Silverton, 'PROPN'),\n",
       " (,, 'PUNCT'),\n",
       " (stockbroker, 'NOUN'),\n",
       " (., 'PUNCT')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[7930].tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ab668b-04d3-44e0-9de2-fae024c35b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.explain('X')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
