{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69072097-9599-4922-ae1a-434b8bd06417",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_newspapers = '../../data/corpora/newspapers_test/'\n",
    "# newspaper_test = 'newspaper_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "229469b7-084b-4003-a5e3-b1047fd6204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import codecs\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fde4ef85-446f-4928-9288-99f7eee5f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_expressions = {\"initials\": r\"\\b([A-Z][.](\\s)?)+\", \"prefixes\": r\"(Mr|St|Mrs|Ms|Dr|Esq|Sec|Secretar)[.]\",\\\n",
    "                     \"addresses\": \"\", \"dates\": \"\", \"line_break\": r\"¬\\n\", \"space\": r\"/s\",\\\n",
    "                     \"dashes\": r\"[-]+\", \"quote_marks\": r\"(“|”)\", \\\n",
    "                     \"months_abrv\": r\"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[.](\\s*(\\d{1,2})(,|\\.)?)?(\\s*\\d+)?\",\\\n",
    "                     \"pennies\": r\"(\\d+[.]?\\s*)[d][.]\", \"months_and_years\": r\"\\d{1,2}[.]\\s*(\\d{4})\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93281673-05ca-42e4-9199-0be58d1c7396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_corpus_of_txts(path=path_to_newspapers):\n",
    "    list_of_filenames_and_dirty_texts = []\n",
    "    for filename in os.listdir(path):\n",
    "        with codecs.open(path + filename, 'r', encoding='utf-8', errors=\"ignore\") as raw_text:\n",
    "            dirty_text = raw_text.read()\n",
    "        list_of_filenames_and_dirty_texts.append((filename, dirty_text))\n",
    "    return list_of_filenames_and_dirty_texts\n",
    "\n",
    "\n",
    "# strip all accented characters:\n",
    "def strip_accents(text):\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    return str(text)\n",
    "\n",
    "def process_periods(text):\n",
    "    # no matchobj needed since this is only called in other processing functions\n",
    "    text = re.sub(r\"[.]\",\"<prd>\", text)\n",
    "    return text\n",
    "\n",
    "def process_periods_to_commas(matchobj):\n",
    "    text = matchobj.group(0)\n",
    "    text = re.sub(r\"[.]\", \",\", text)\n",
    "    return text\n",
    "\n",
    "# processing functions for regex calls in preprocess_text() function\n",
    "# process initials for regex, and return a format that we can identify\n",
    "def process_initials(matchobj):\n",
    "    text = matchobj.group(0)\n",
    "    text = process_periods(text)\n",
    "    text = re.sub(r\"\\s*\", \"\", text)\n",
    "    text = text + \" \"\n",
    "    return text\n",
    "\n",
    "def process_months_abrv(matchobj):\n",
    "    text = matchobj.group(0)\n",
    "    text = process_periods(text)\n",
    "    # text = \" <date>\"+text+\"<date> \"\n",
    "    return text\n",
    "\n",
    "def process_pennies(matchobj):\n",
    "    text = matchobj.group(0)\n",
    "    text = re.sub(r\"d[.]?\",\"pennies\", text)\n",
    "    text = process_periods(text)\n",
    "    return text\n",
    "\n",
    "# combine it together\n",
    "def preprocess_text(text):\n",
    "    # remove all the line breaks created by newspaper processor\n",
    "    text = re.sub(regex_expressions[\"line_break\"],\"\", text)\n",
    "    # marking initials:\n",
    "    text = re.sub(regex_expressions[\"initials\"], process_initials, text)\n",
    "    # process titles:\n",
    "    text = re.sub(regex_expressions[\"prefixes\"],\"\\\\1<prd>\", text, flags=re.IGNORECASE)\n",
    "    # process month abbreviations:\n",
    "    text = re.sub(regex_expressions[\"months_abrv\"], process_months_abrv, text, flags=re.IGNORECASE)\n",
    "    # process instances of months [period] year:\n",
    "    text = re.sub(regex_expressions[\"months_and_years\"], process_periods_to_commas, text)\n",
    "    # process instances of \"No.\"\n",
    "    text = re.sub(r\"(No|Nos)[.]\",\"number\", text, flags=re.IGNORECASE)\n",
    "    # strip all dashes:\n",
    "    text = re.sub(regex_expressions[\"dashes\"], \" \", text)\n",
    "    # transform all quotes to ' \" ':\n",
    "    text = re.sub(regex_expressions[\"quote_marks\"], '\"', text)\n",
    "    # strip all pennies \"XX d.\" in the text:\n",
    "    text = re.sub(regex_expressions[\"pennies\"], process_pennies, text)\n",
    "    # strip all accents from the text:\n",
    "    text = strip_accents(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa2aee5-8d01-4f23-b83e-7f0453b372a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_tokenized_sent(sent):\n",
    "    # removing newline notations\n",
    "    clean_sent = re.sub('\\n', ' ', sent)\n",
    "    clean_sent = re.sub('\\r', ' ', clean_sent)\n",
    "    # transforming multiple spaces to one space\n",
    "    clean_sent = re.sub('\\s+',' ', clean_sent)\n",
    "    split_sentence = clean_sent.split()\n",
    "    \n",
    "    # transform all the words that are completely uppercase to lowercase\n",
    "    for index, word in enumerate(split_sentence):\n",
    "        if (word.isupper()):\n",
    "            new_word = word.lower()\n",
    "            split_sentence[index] = new_word\n",
    "    clean_sent = \" \".join(split_sentence)\n",
    "    \n",
    "    # put back the periods:\n",
    "    clean_sent = re.sub(\"<prd>\", \".\", clean_sent)\n",
    "    # clean_sent = clean_sent.lower()\n",
    "    return clean_sent\n",
    "\n",
    "\n",
    "### Not needed if done in df\n",
    "def clean_tokenized_list(sent_list):\n",
    "    cleaned_tokenized_sentences = []\n",
    "    for sent in sent_list:\n",
    "        clean_set = clean_tokenized_sent(sent)\n",
    "        cleaned_tokenized_sentences.append(clean_set)\n",
    "    return cleaned_tokenized_sentences\n",
    "\n",
    "def process_dirty_texts_to_df(list_of_filenames_and_dirty_texts):\n",
    "    filenames = []\n",
    "    cleaned_texts = []\n",
    "    cleaned_corpus_as_dictionary = {}\n",
    "    for filename, dirty_text in list_of_filenames_and_dirty_texts:\n",
    "        preprocessed_text = preprocess_text(dirty_text)\n",
    "        tokenized_sentences = sent_tokenize(preprocessed_text)\n",
    "        cleaned_tokenized_sentences = clean_tokenized_list(tokenized_sentences)\n",
    "        for clean_tokenized_sentence in cleaned_tokenized_sentences:\n",
    "            filenames.append(filename)\n",
    "            cleaned_texts.append(clean_tokenized_sentence)\n",
    "    cleaned_corpus_as_dictionary['file_names'] = filenames\n",
    "    cleaned_corpus_as_dictionary['sentences'] = cleaned_texts\n",
    "    \n",
    "    df = pd.DataFrame(cleaned_corpus_as_dictionary)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bbe18ba6-8f69-44d0-bd58-96a55f34eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS try:\n",
    "# first loading english language support\n",
    "\n",
    "# faster but less accurate model:\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# slower but more accurate model:\n",
    "# download it first\n",
    "# !python -m spacy download en_core_web_lg\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def get_pos_counts_from_tagged_sentence(analyzed_sent):\n",
    "    pos_counts = []\n",
    "    pos_counts_raw = analyzed_sent.count_by(spacy.attrs.IDS['POS'])\n",
    "    for pos, count in pos_counts_raw.items():\n",
    "        tag = analyzed_sent.vocab[pos].text\n",
    "        pos_count = (tag, count)\n",
    "        pos_counts.append(pos_count)\n",
    "    # return a list of pos_counts\n",
    "    return pos_counts\n",
    "\n",
    "def pos_tag_sentence(sent):\n",
    "    tagged_sentence = []\n",
    "    analyzed_sent = nlp(sent, disable = ['ner'])\n",
    "    # getting the complete tokenized sentence\n",
    "    for token in analyzed_sent:\n",
    "        tagged_word = (token, token.pos_)\n",
    "        tagged_sentence.append(tagged_word)\n",
    "    pos_counts = get_pos_counts_from_tagged_sentence(analyzed_sent)\n",
    "    # return a tuple of both\n",
    "    return (tagged_sentence, pos_counts)\n",
    "\n",
    "def pos_tag_list_of_sentences(list_of_cleaned_sentences):\n",
    "    pos_tagged_text = []\n",
    "    for sent in list_of_cleaned_sentences:\n",
    "        tagged_sent = pos_tag_sentence(sent)\n",
    "        pos_tagged_text.append(tagged_sent)\n",
    "    # returns a list of tuples\n",
    "    return pos_tagged_text\n",
    "\n",
    "\n",
    "def pos_tag_texts_from_df(df, sentences_column='sentences'):\n",
    "    df['tagged_sentences'] = ''\n",
    "    df['pos_counts'] = ''\n",
    "    for index, row in df.iterrows():\n",
    "        sentence = row[sentences_column]\n",
    "        tagged_sentence, pos_counts = pos_tag_sentence(sentence)\n",
    "        df.at[index, 'tagged_sentences'] = tagged_sentence\n",
    "        df.at[index, 'pos_counts'] = pos_counts\n",
    "        # print(tagged_sentence)\n",
    "        # if index >= 10:\n",
    "        #     break\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28cd51a-8a84-47bd-a01e-3473919198b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "40ce0572-03fa-4e08-9fb7-356c2cc2a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get total POS counts from df once ready\n",
    "def get_total_pos_counts(list_of_pos_counts):\n",
    "    for index, pos_counts in enumerate(list_of_pos_counts):\n",
    "        for tag, count in pos_counts:\n",
    "            print(tag)\n",
    "            print(count)\n",
    "        if index >= 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3529571-868c-4a9d-9144-11dd5228e15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>newspaper_test2.txt</td>\n",
       "      <td>afe F.E. write immediately.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>newspaper_test2.txt</td>\n",
       "      <td>In London de vue de are 1 himney top.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>newspaper_test2.txt</td>\n",
       "      <td>E.E.F. , alive, weil, and at Hythe.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>newspaper_test2.txt</td>\n",
       "      <td>a Happy New Year to you sil.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>newspaper_test2.txt</td>\n",
       "      <td>ames.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            file_names                              sentences\n",
       "0  newspaper_test2.txt            afe F.E. write immediately.\n",
       "1  newspaper_test2.txt  In London de vue de are 1 himney top.\n",
       "2  newspaper_test2.txt    E.E.F. , alive, weil, and at Hythe.\n",
       "3  newspaper_test2.txt           a Happy New Year to you sil.\n",
       "4  newspaper_test2.txt                                  ames."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creating a dictionary of all the texts with keys as their filenames\n",
    "dirty_texts = input_corpus_of_txts(path=path_to_newspapers)\n",
    "# print(dirty_texts)\n",
    "df = process_dirty_texts_to_df(dirty_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "797dc42f-bf80-4aa3-a2db-a61158b2d352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_names</th>\n",
       "      <th>sentences</th>\n",
       "      <th>tagged_sentences</th>\n",
       "      <th>pos_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>newspaper_test2.txt</td>\n",
       "      <td>afe F.E. write immediately.</td>\n",
       "      <td>[(afe, PROPN), (F.E., PROPN), (write, VERB), (...</td>\n",
       "      <td>[(PROPN, 2), (VERB, 1), (ADV, 1), (PUNCT, 1)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>newspaper_test2.txt</td>\n",
       "      <td>In London de vue de are 1 himney top.</td>\n",
       "      <td>[(In, ADP), (London, PROPN), (de, X), (vue, X)...</td>\n",
       "      <td>[(ADP, 1), (PROPN, 1), (X, 3), (AUX, 1), (NUM,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>newspaper_test2.txt</td>\n",
       "      <td>E.E.F. , alive, weil, and at Hythe.</td>\n",
       "      <td>[(E.E.F., PROPN), (,, PUNCT), (alive, ADJ), (,...</td>\n",
       "      <td>[(PROPN, 3), (PUNCT, 4), (ADJ, 1), (CCONJ, 1),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>newspaper_test2.txt</td>\n",
       "      <td>a Happy New Year to you sil.</td>\n",
       "      <td>[(a, DET), (Happy, PROPN), (New, PROPN), (Year...</td>\n",
       "      <td>[(DET, 1), (PROPN, 3), (ADP, 1), (PRON, 1), (N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>newspaper_test2.txt</td>\n",
       "      <td>ames.</td>\n",
       "      <td>[(ames, PROPN), (., PUNCT)]</td>\n",
       "      <td>[(PROPN, 1), (PUNCT, 1)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            file_names                              sentences  \\\n",
       "0  newspaper_test2.txt            afe F.E. write immediately.   \n",
       "1  newspaper_test2.txt  In London de vue de are 1 himney top.   \n",
       "2  newspaper_test2.txt    E.E.F. , alive, weil, and at Hythe.   \n",
       "3  newspaper_test2.txt           a Happy New Year to you sil.   \n",
       "4  newspaper_test2.txt                                  ames.   \n",
       "\n",
       "                                    tagged_sentences  \\\n",
       "0  [(afe, PROPN), (F.E., PROPN), (write, VERB), (...   \n",
       "1  [(In, ADP), (London, PROPN), (de, X), (vue, X)...   \n",
       "2  [(E.E.F., PROPN), (,, PUNCT), (alive, ADJ), (,...   \n",
       "3  [(a, DET), (Happy, PROPN), (New, PROPN), (Year...   \n",
       "4                        [(ames, PROPN), (., PUNCT)]   \n",
       "\n",
       "                                          pos_counts  \n",
       "0      [(PROPN, 2), (VERB, 1), (ADV, 1), (PUNCT, 1)]  \n",
       "1  [(ADP, 1), (PROPN, 1), (X, 3), (AUX, 1), (NUM,...  \n",
       "2  [(PROPN, 3), (PUNCT, 4), (ADJ, 1), (CCONJ, 1),...  \n",
       "3  [(DET, 1), (PROPN, 3), (ADP, 1), (PRON, 1), (N...  \n",
       "4                           [(PROPN, 1), (PUNCT, 1)]  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pos_tag_texts_from_df(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef4ae7-e80a-4075-a891-dd08b7f41a79",
   "metadata": {},
   "outputs": [],
   "source": [
    " # tagged_sentence = pos_tag_list_of_sentences(cleaned_tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4cd53a-153c-4802-be1f-c4abc5e29411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tagged_sentence[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ab668b-04d3-44e0-9de2-fae024c35b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.explain('X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d197041c-0fb0-4283-9616-be8a6084924d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3372b421-08e8-4bd9-bdec-de3b2cca61d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dirty_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15ec28-125d-4828-9157-a10db1e2b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, sent in enumerate(cleaned_tokenized_sentences):\n",
    "    if index <= 50:\n",
    "        tagged_sentence, pos_counts = pos_tag_sentence(sent)\n",
    "        print(index, end=\": \")\n",
    "        print(sent)\n",
    "        print(tagged_sentence)\n",
    "        print(pos_counts, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa327e-c840-4f3c-a2de-1727a7efa37c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
