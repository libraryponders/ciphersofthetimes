{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce879b8b-0817-45ac-ae50-14a47f7c0d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def progress(count, total, status=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "    \n",
    "    #old: percents = round(100.0 * count / float(total), 1)\n",
    "    percents = round(100.1 * count / float(total), 1)\n",
    "    bar = '#' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n",
    "    sys.stdout.flush()  # As suggested by Rom Ruben (see: http://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console/27871113#comment50529068_27871113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f97a621c-bd24-4d3f-8e0a-0a88c4e8b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the Times XML parser p1\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "#defines the file's directory for the xml files\n",
    "path = './2022_01_31_Newspaper_Parser/PC-bu/xml_sheets/'\n",
    "\n",
    "def getMainAttribute(tree, writer, minOCR=40.0):\n",
    "\n",
    "    #get root of xml file\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    #issue number\n",
    "    # print(root.tag + root.attrib['ID'])\n",
    "    issueID = root.tag+root.attrib['ID']\n",
    "\n",
    "    #get date\n",
    "    full_date = root.find('.//composed').text\n",
    "    # print(date)\n",
    "\n",
    "    #get width and height of the paper\n",
    "    width = root.find('.//pageid').attrib['width']\n",
    "    height = root.find('.//pageid').attrib['height']\n",
    "\n",
    "    #Articles is defined as a list (we get all the articles in the paper)\n",
    "    Articles = root.findall(\".//article[@type='Article']\")\n",
    "\n",
    "    #loop through the articles\n",
    "    for Article in Articles:\n",
    "        #check for ocr scores\n",
    "        ocrScore = float(Article.find(\".//ocr\").text)    \n",
    "        if ocrScore < minOCR:\n",
    "            break\n",
    "        pageNumber = Article.find(\".//pi[@pgref='1']\")\n",
    "           #breaks loop if we are not on the first page\n",
    "        if pageNumber is None:\n",
    "            break\n",
    "\n",
    "        title = Article.find(\".//ti\").text\n",
    "        # print(title)\n",
    "\n",
    "        TextInArticle = Article.find(\".//text\")\n",
    "        #get <text.cr>\n",
    "        TextInput = ''\n",
    "        for TextContainer in TextInArticle:\n",
    "            #get <pg> and <p>\n",
    "            for Paragraph in TextContainer:\n",
    "                if Paragraph.tag == \"pg\":\n",
    "                    positionValues = [int(lbtr) for lbtr in Paragraph.attrib['pos'].split(',')]\n",
    "                    #if Paragraph.attrib['clipref'] != \"2\":\n",
    "                    #    break\n",
    "\n",
    "                #subparagraph gives us the wd positions\n",
    "                \n",
    "                for SubParagraph in Paragraph:\n",
    "                    \n",
    "                                #[0] = left\n",
    "                                #[1] = bottom\n",
    "                                #[2] = right\n",
    "                                #[3] = top\n",
    "                                #in case it's needed\n",
    "                    subPositionValues = [int(lbtr) for lbtr in SubParagraph.attrib['pos'].split(',')]\n",
    "                    if(int(subPositionValues[2]) <= (int(width)/6)+5):\n",
    "                        # print(\"1st BREAK\")\n",
    "                        break    \n",
    "                    elif(int(subPositionValues[2]) >= (3*(int(width)/6))+5):\n",
    "                        # print(\"2nd BREAK\")\n",
    "                        break\n",
    "                    else:\n",
    "                        # print(\"3rd BREAK\")\n",
    "                        # print(SubParagraph.text)\n",
    "                        TextInput += SubParagraph.text + ' '\n",
    "                    #print(SubParagraph.text, end=' ')    \n",
    "                    #print(positionValues[0],positionValues[1],positionValues[2],positionValues[3])\n",
    "                #print(TextInput)\n",
    "        writer.writerow([issueID, full_date, title, ocrScore, TextInput])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b3fbac5-3fad-40f1-87ee-0c8ef0952007",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_newspaper_XMLs_to_df(name_of_csv='testrun', path_to_XML_sheets=path, minOCR=40.0):\n",
    "    with open(name_of_csv+'.csv', 'w', newline='', encoding=\"utf8\", errors='ignore') as csvFile:\n",
    "        i = 0\n",
    "        writer = csv.writer(csvFile) \n",
    "        writer.writerow([\"issueID\", \"full_date\", \"title\", \"ocrScore\", \"text\"])\n",
    "        for files in os.listdir(path_to_XML_sheets):\n",
    "            # progress(i, len(os.listdir(path)))\n",
    "            if not files.endswith('.xml'):\n",
    "                continue\n",
    "            fullname = os.path.join(path_to_XML_sheets, files)\n",
    "            tree  = ET.parse(fullname)\n",
    "            getMainAttribute(tree, writer, minOCR)\n",
    "            i += 1\n",
    "            progress(i, len(os.listdir(path)))\n",
    "    #group duplicate rows\n",
    "    df = pd.read_csv(r'testrun.csv', delimiter=',')\n",
    "    # new: df = pd.read_csv('testrun.csv', delimiter=',')\n",
    "    #drop nan values\n",
    "    df.replace(['None', 'nan'], np.nan, inplace=True)\n",
    "    df.dropna(how='any', inplace=True)\n",
    "    return df\n",
    "\n",
    "def give_me_just_the_year_from_date(df, date_column='full_date'):\n",
    "    for index, row in df.iterrows():\n",
    "        full_date = row[date_column]\n",
    "        full_date_split = full_date.split()\n",
    "        df.at[index, 'text_year'] = full_date_split[2] \n",
    "    return df\n",
    "#reformat for a new output csv\n",
    "# out = df.astype(str).groupby(['date', 'issueID', 'title']).agg(', '.join)\n",
    "# print(out)\n",
    "# out.to_csv('formatted_output.csv')\n",
    "# out.head(-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0f35c8b-6975-47b3-a5fc-0257421d825b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[############################################################] 100.1% ...\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issueID</th>\n",
       "      <th>full_date</th>\n",
       "      <th>title</th>\n",
       "      <th>ocrScore</th>\n",
       "      <th>text</th>\n",
       "      <th>text_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>issueN0029851</td>\n",
       "      <td>September 1, 1880</td>\n",
       "      <td>Births</td>\n",
       "      <td>48.95</td>\n",
       "      <td>BIRTHS.1</td>\n",
       "      <td>1880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>issueN0027091</td>\n",
       "      <td>August 9, 1872</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>58.25</td>\n",
       "      <td>DEATHS. On'the 3d inst.. at Ostende, deeply la...</td>\n",
       "      <td>1872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>issueN0027091</td>\n",
       "      <td>August 9, 1872</td>\n",
       "      <td>L. G.-Have an important communication for you.</td>\n",
       "      <td>51.69</td>\n",
       "      <td>G.-Have an iTmuporta3nt communication for you....</td>\n",
       "      <td>1872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>issueN0014350</td>\n",
       "      <td>December 10, 1831</td>\n",
       "      <td>FOR CALCUTTA, will land passengers at Madras, ...</td>\n",
       "      <td>47.82</td>\n",
       "      <td>OS P 'ARCEMENT DEED, consisting of tw,slt,betw...</td>\n",
       "      <td>1831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>issueN0012785</td>\n",
       "      <td>December 9, 1826</td>\n",
       "      <td>SHIP LOWTHER CASTLE, East Indiaman.-The CREDIT...</td>\n",
       "      <td>45.49</td>\n",
       "      <td>1~OTJD; Mae, aGtNTLaMA's~ ~NRY~~DIGS~ WHIEI d?...</td>\n",
       "      <td>1826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         issueID          full_date  \\\n",
       "0  issueN0029851  September 1, 1880   \n",
       "3  issueN0027091     August 9, 1872   \n",
       "4  issueN0027091     August 9, 1872   \n",
       "5  issueN0014350  December 10, 1831   \n",
       "9  issueN0012785   December 9, 1826   \n",
       "\n",
       "                                               title  ocrScore  \\\n",
       "0                                             Births     48.95   \n",
       "3                                             Deaths     58.25   \n",
       "4     L. G.-Have an important communication for you.     51.69   \n",
       "5  FOR CALCUTTA, will land passengers at Madras, ...     47.82   \n",
       "9  SHIP LOWTHER CASTLE, East Indiaman.-The CREDIT...     45.49   \n",
       "\n",
       "                                                text text_year  \n",
       "0                                          BIRTHS.1       1880  \n",
       "3  DEATHS. On'the 3d inst.. at Ostende, deeply la...      1872  \n",
       "4  G.-Have an iTmuporta3nt communication for you....      1872  \n",
       "5  OS P 'ARCEMENT DEED, consisting of tw,slt,betw...      1831  \n",
       "9  1~OTJD; Mae, aGtNTLaMA's~ ~NRY~~DIGS~ WHIEI d?...      1826  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = write_newspaper_XMLs_to_df()\n",
    "df = give_me_just_the_year_from_date(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b821d814-321c-4a9c-97a1-c2d1c03401cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18ca2faa-9791-4c5e-bf28-7d2a7ecc90e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "dashes = \"(--)+\"\n",
    "df['sentences'] = ''\n",
    "for index, row in df.iterrows():\n",
    "    dirty_text = row['text']\n",
    "    dirty_text = re.sub(prefixes,\"\\\\1<prd>\", dirty_text)\n",
    "    dirty_text = re.sub(dashes, ' \\\\1 ', dirty_text)\n",
    "    dirty_text = re.sub('(“|”)', '\"', dirty_text)\n",
    "    clean_text_list = sent_tokenize(dirty_text)\n",
    "\n",
    "    clean_sentences = []\n",
    "    for sent in clean_text_list:\n",
    "        # res.append(re.sub('\\n', '', sent))\n",
    "        # removing newline notations\n",
    "        clean_sent = re.sub('\\n', ' ', sent)\n",
    "        clean_sent = re.sub('\\r', ' ', clean_sent)\n",
    "        # transforming multiple spaces to one space\n",
    "        clean_sent = re.sub('\\s+',' ', clean_sent)\n",
    "        clean_sentences.append(clean_sent)\n",
    "    df.at[index, 'sentences']= clean_sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99a7ecb2-9d43-474f-8433-e3d34262a744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issueID</th>\n",
       "      <th>full_date</th>\n",
       "      <th>title</th>\n",
       "      <th>ocrScore</th>\n",
       "      <th>text</th>\n",
       "      <th>text_year</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>issueN0029851</td>\n",
       "      <td>September 1, 1880</td>\n",
       "      <td>Births</td>\n",
       "      <td>48.95</td>\n",
       "      <td>BIRTHS.1</td>\n",
       "      <td>1880</td>\n",
       "      <td>[BIRTHS.1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>issueN0027091</td>\n",
       "      <td>August 9, 1872</td>\n",
       "      <td>Deaths</td>\n",
       "      <td>58.25</td>\n",
       "      <td>DEATHS. On'the 3d inst.. at Ostende, deeply la...</td>\n",
       "      <td>1872</td>\n",
       "      <td>[DEATHS., On'the 3d inst.. at Ostende, deeply ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>issueN0027091</td>\n",
       "      <td>August 9, 1872</td>\n",
       "      <td>L. G.-Have an important communication for you.</td>\n",
       "      <td>51.69</td>\n",
       "      <td>G.-Have an iTmuporta3nt communication for you....</td>\n",
       "      <td>1872</td>\n",
       "      <td>[G.-Have an iTmuporta3nt communication for you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>issueN0014350</td>\n",
       "      <td>December 10, 1831</td>\n",
       "      <td>FOR CALCUTTA, will land passengers at Madras, ...</td>\n",
       "      <td>47.82</td>\n",
       "      <td>OS P 'ARCEMENT DEED, consisting of tw,slt,betw...</td>\n",
       "      <td>1831</td>\n",
       "      <td>[OS P 'ARCEMENT DEED, consisting of tw,slt,bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>issueN0012785</td>\n",
       "      <td>December 9, 1826</td>\n",
       "      <td>SHIP LOWTHER CASTLE, East Indiaman.-The CREDIT...</td>\n",
       "      <td>45.49</td>\n",
       "      <td>1~OTJD; Mae, aGtNTLaMA's~ ~NRY~~DIGS~ WHIEI d?...</td>\n",
       "      <td>1826</td>\n",
       "      <td>[1~OTJD; Mae, aGtNTLaMA's~ ~NRY~~DIGS~ WHIEI d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         issueID          full_date  \\\n",
       "0  issueN0029851  September 1, 1880   \n",
       "3  issueN0027091     August 9, 1872   \n",
       "4  issueN0027091     August 9, 1872   \n",
       "5  issueN0014350  December 10, 1831   \n",
       "9  issueN0012785   December 9, 1826   \n",
       "\n",
       "                                               title  ocrScore  \\\n",
       "0                                             Births     48.95   \n",
       "3                                             Deaths     58.25   \n",
       "4     L. G.-Have an important communication for you.     51.69   \n",
       "5  FOR CALCUTTA, will land passengers at Madras, ...     47.82   \n",
       "9  SHIP LOWTHER CASTLE, East Indiaman.-The CREDIT...     45.49   \n",
       "\n",
       "                                                text text_year  \\\n",
       "0                                          BIRTHS.1       1880   \n",
       "3  DEATHS. On'the 3d inst.. at Ostende, deeply la...      1872   \n",
       "4  G.-Have an iTmuporta3nt communication for you....      1872   \n",
       "5  OS P 'ARCEMENT DEED, consisting of tw,slt,betw...      1831   \n",
       "9  1~OTJD; Mae, aGtNTLaMA's~ ~NRY~~DIGS~ WHIEI d?...      1826   \n",
       "\n",
       "                                           sentences  \n",
       "0                                         [BIRTHS.1]  \n",
       "3  [DEATHS., On'the 3d inst.. at Ostende, deeply ...  \n",
       "4  [G.-Have an iTmuporta3nt communication for you...  \n",
       "5  [OS P 'ARCEMENT DEED, consisting of tw,slt,bet...  \n",
       "9  [1~OTJD; Mae, aGtNTLaMA's~ ~NRY~~DIGS~ WHIEI d...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e335bc78-ada1-4812-9971-e6d085dfaf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48.95, 58.25, 51.69, 47.82, 45.49, 43.19, 53.33, 47.45, 44.01, 49.07, 66.11, 62.18, 62.04, 60.84, 56.4, 60.87, 54.93, 66.33, 56.48, 65.99, 50.28, 56.79, 69.26, 67.03, 52.73, 47.99, 50.97, 53.38, 54.15, 51.7]\n",
      "55.19000000000001\n"
     ]
    }
   ],
   "source": [
    "print(df.ocrScore.values.tolist())\n",
    "print(df.ocrScore.values.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d637b2-f46a-4308-b933-af881a7d51f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## POS tagging:\n",
    "\n",
    "def progress(count, total, status=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "    percents = round(100.1 * count / float(total), 1)\n",
    "    bar = '#' * filled_len + '-' * (bar_len - filled_len)\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n",
    "    sys.stdout.flush() \n",
    "    \n",
    "# first loading english language support\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "## Takes in a dataframe and clean text column (as string), and returns the df with POS tags for all the texts\n",
    "## Multiple columns are created, one for each POS tag, and one that contains all POS tags (I did this to more easily\n",
    "## be able to grab POS percentages afterward)\n",
    "def get_POS_tags_for_text_in_df(df, text_row_to_analyze='text'):\n",
    "    # setting up column for pos counts\n",
    "    df['all_pos_counts'] = ''\n",
    "    df[\"parts_of_speech_total_count\"] = ''\n",
    "    # loop through df and get all POS tags:\n",
    "    for index, row in df.iterrows():\n",
    "        # show progress\n",
    "        progress(index, len(df.index))\n",
    "        \n",
    "        # grab text\n",
    "        text = row[text_row_to_analyze]\n",
    "        \n",
    "        # this is a memory buffer, to extend max length of available ram according to the text being analyzed\n",
    "        # https://datascience.stackexchange.com/questions/38745/increasing-spacy-max-nlp-limit\n",
    "        nlp.max_length = len(text) + 100\n",
    "        \n",
    "        # disable modules not in use to save memory\n",
    "        analyzed_doc = nlp(text, disable = ['ner'])\n",
    "        \n",
    "        # grabbing all pos counts in the text in non-human readable format\n",
    "        pos_counts_in_text = analyzed_doc.count_by(spacy.attrs.IDS['POS'])\n",
    "        \n",
    "        # setting up list to render pos hashes in human readable format:\n",
    "        human_readable_pos_count_list = []\n",
    "        \n",
    "        # iterating through counts to make hashes human readable:\n",
    "        for pos, count in pos_counts_in_text.items():\n",
    "            human_readable_tag = analyzed_doc.vocab[pos].text\n",
    "            # rendering as list to input back into df\n",
    "            human_readable_tag_and_count = list((human_readable_tag, count))\n",
    "            human_readable_pos_count_list.append(human_readable_tag_and_count)\n",
    "        # looping through the human readable counts, assigning their label to the column\n",
    "        # and the count to the row for each pos tag\n",
    "        for element in human_readable_pos_count_list:\n",
    "            df.at[index, 'POS_' + str(element[0])+'_count'] = element[1]\n",
    "        \n",
    "        # placing all the pos counts for each text in the all_pos_counts column\n",
    "        df.at[index, 'all_pos_counts'] = human_readable_pos_count_list\n",
    "        \n",
    "        \n",
    "    df = df.fillna(0)\n",
    "    # getting POS percentages for each POS tag in texts\n",
    "    # There are much easier and more efficient ways to do this rather than looping over the entire df again but we were pressed for time...\n",
    "    # TODO: integrate this loop into previous loop\n",
    "    for index, row in df.iterrows():\n",
    "        total = 0.0\n",
    "        for name in df.columns.values.tolist():\n",
    "            if name.startswith(\"POS_\"):\n",
    "                # get total POS elements count for sanity\n",
    "                total += row[name]\n",
    "        try:\n",
    "            df.at[index, \"parts_of_speech_total_count\"] = int(total)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        for name in df.columns.values.tolist():\n",
    "            if name.startswith(\"POS_\"):\n",
    "                # assign new name for column\n",
    "                new_name = \"%\" + name\n",
    "                # get % of total POS in text\n",
    "                if total != 0:\n",
    "                    percentage = round((row[name] / total) * float(100), 3)\n",
    "                else:\n",
    "                    print(row[text_row_to_analyze])\n",
    "                # if this is the first index, create the column name to avoid errors\n",
    "                if index == 0:\n",
    "                    df[new_name] = 0.0\n",
    "                df.at[index, new_name] = percentage\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd6c5d21-f677-4538-a986-8c0de055a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.to_csv(\"test_each_AC_same_row.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd00539-abec-4a49-a9e0-5eb42d26c659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
