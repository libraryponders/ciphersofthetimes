{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4c21037-1903-41d1-b4ea-e511d7274c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports, utility, and cleaning functions\n",
    "\n",
    "# required imports:\n",
    "import codecs\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# project functions:\n",
    "# from utility_code import *\n",
    "import sys\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "# faster but less accurate model:\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# slower but more accurate model:\n",
    "# download it first\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e0463e6-96ad-495e-8afe-59f7fd258d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## From utility_code.py\n",
    "## Progress bar to view the progress of lengthy processes\n",
    "# As suggested by Rom Ruben (see: http://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console/27871113#comment50529068_27871113)\n",
    "def progress(count, total, status=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "    percents = round(100.1 * count / float(total), 1)\n",
    "    bar = '#' * filled_len + '-' * (bar_len - filled_len)\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n",
    "    sys.stdout.flush() \n",
    "\n",
    "# for full output:\n",
    "def output_full(df, path_to_spreadsheets):\n",
    "    spreadsheet_name = input(\"[=] Please input desired spreadsheet name: \")\n",
    "    df.to_csv(path_to_spreadsheets + spreadsheet_name + '.csv')\n",
    "    print(spreadsheet_name + ' was saved in '+str(path_to_spreadsheets) + f\" as {spreadsheet_name}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bda2761f-8ca0-4925-8f51-9c7f2a65bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## All cleaning functions and regex\n",
    "\n",
    "### regex:\n",
    "regex_expressions = {\"initials\": r\"\\b([A-Z][.](\\s)?)+\", \"prefixes\": r\"(Mr|St|Mrs|Ms|Dr|Esq|Sec|Secretar)[.]\",\\\n",
    "                     \"addresses\": \"\", \"dates\": \"\", \"line_break\": r\"¬\\n\", \"space\": r\"/s\",\\\n",
    "                     \"dashes\": r\"[-]+\", \"quote_marks\": r\"(“|”)\", \\\n",
    "                     \"months_abrv\": r\"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[.](\\s*(\\d{1,2})(,|\\.)?)?(\\s*\\d+)?\",\\\n",
    "                     \"pennies\": r\"(\\d+[.]?\\s*)[d][.]\", \"months_and_years\": r\"\\d{1,2}[.]\\s*(\\d{4})\"}\n",
    "\n",
    "\n",
    "def input_corpus_of_txts(path):\n",
    "    list_of_filenames_and_dirty_texts = []\n",
    "    for filename in os.listdir(path):\n",
    "        with codecs.open(path + filename, 'r', encoding='utf-8', errors=\"ignore\") as raw_text:\n",
    "            dirty_text = raw_text.read()\n",
    "        list_of_filenames_and_dirty_texts.append((filename, dirty_text))\n",
    "    return list_of_filenames_and_dirty_texts\n",
    "\n",
    "\n",
    "# strip all accented characters:\n",
    "def strip_accents(text):\n",
    "    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    return str(text)\n",
    "\n",
    "def process_periods(text):\n",
    "    # no matchobj needed since this is only called in other processing functions\n",
    "    text = re.sub(r\"[.]\",\"<prd>\", text)\n",
    "    return text\n",
    "\n",
    "def process_periods_to_commas(matchobj):\n",
    "    text = matchobj.group(0)\n",
    "    text = re.sub(r\"[.]\", \",\", text)\n",
    "    return text\n",
    "\n",
    "# processing functions for regex calls in preprocess_text() function\n",
    "# process initials for regex, and return a format that we can identify\n",
    "def process_initials(matchobj):\n",
    "    text = matchobj.group(0)\n",
    "    text = process_periods(text)\n",
    "    text = re.sub(r\"\\s*\", \"\", text)\n",
    "    text = text + \" \"\n",
    "    return text\n",
    "\n",
    "def process_months_abrv(matchobj):\n",
    "    text = matchobj.group(0)\n",
    "    text = process_periods(text)\n",
    "    # text = \" <date>\"+text+\"<date> \"\n",
    "    return text\n",
    "\n",
    "def process_pennies(matchobj):\n",
    "    text = matchobj.group(0)\n",
    "    text = re.sub(r\"d[.]?\",\"pennies\", text)\n",
    "    text = process_periods(text)\n",
    "    return text\n",
    "\n",
    "# combine it together\n",
    "def preprocess_text(text):\n",
    "    # remove all the line breaks created by newspaper processor\n",
    "    text = re.sub(regex_expressions[\"line_break\"],\"\", text)\n",
    "    # marking initials:\n",
    "    text = re.sub(regex_expressions[\"initials\"], process_initials, text)\n",
    "    # process titles:\n",
    "    text = re.sub(regex_expressions[\"prefixes\"],\"\\\\1<prd>\", text, flags=re.IGNORECASE)\n",
    "    # process month abbreviations:\n",
    "    text = re.sub(regex_expressions[\"months_abrv\"], process_months_abrv, text, flags=re.IGNORECASE)\n",
    "    # process instances of months [period] year:\n",
    "    text = re.sub(regex_expressions[\"months_and_years\"], process_periods_to_commas, text)\n",
    "    # process instances of \"No.\"\n",
    "    text = re.sub(r\"(No|Nos)[.]\",\"number\", text, flags=re.IGNORECASE)\n",
    "    # strip all dashes:\n",
    "    text = re.sub(regex_expressions[\"dashes\"], \" \", text)\n",
    "    # transform all quotes to ' \" ':\n",
    "    text = re.sub(regex_expressions[\"quote_marks\"], '\"', text)\n",
    "    # strip all pennies \"XX d.\" in the text:\n",
    "    text = re.sub(regex_expressions[\"pennies\"], process_pennies, text)\n",
    "    # strip all accents from the text:\n",
    "    text = strip_accents(text)\n",
    "    # print(\"[-] Finished processing linebreaks, initials, prefixes, months, years, numbers, dashes, quotations marks, and pennies symbols...\")\n",
    "    return text\n",
    "\n",
    "def clean_tokenized_sent(sent):\n",
    "    # removing newline notations\n",
    "    clean_sent = re.sub('\\n', ' ', sent)\n",
    "    clean_sent = re.sub('\\r', ' ', clean_sent)\n",
    "    # transforming multiple spaces to one space\n",
    "    clean_sent = re.sub('\\s+',' ', clean_sent)\n",
    "    split_sentence = clean_sent.split()\n",
    "    \n",
    "    # transform all the words that are completely uppercase to lowercase\n",
    "    for index, word in enumerate(split_sentence):\n",
    "        if (word.isupper()):\n",
    "            new_word = word.lower()\n",
    "            split_sentence[index] = new_word\n",
    "    clean_sent = \" \".join(split_sentence)\n",
    "    \n",
    "    # put back the periods:\n",
    "    clean_sent = re.sub(\"<prd>\", \".\", clean_sent)\n",
    "    # clean_sent = clean_sent.lower()\n",
    "    return clean_sent\n",
    "\n",
    "### Not needed if done in df\n",
    "def clean_tokenized_list(sent_list):\n",
    "    cleaned_tokenized_sentences = []\n",
    "    for sent in sent_list:\n",
    "        clean_set = clean_tokenized_sent(sent)\n",
    "        cleaned_tokenized_sentences.append(clean_set)\n",
    "    return cleaned_tokenized_sentences\n",
    "\n",
    "def process_dirty_texts_to_df(list_of_filenames_and_dirty_texts):\n",
    "    cleaned_texts = []\n",
    "    cleaned_corpus_as_dictionary = {}\n",
    "    for filename, dirty_text in list_of_filenames_and_dirty_texts:\n",
    "        preprocessed_text = preprocess_text(dirty_text)\n",
    "        tokenized_sentences = sent_tokenize(preprocessed_text)\n",
    "        cleaned_tokenized_sentences = clean_tokenized_list(tokenized_sentences)\n",
    "        relative_sentence_index = 0\n",
    "        for clean_tokenized_sentence in cleaned_tokenized_sentences:\n",
    "            tupled_files = (filename, clean_tokenized_sentence, relative_sentence_index)\n",
    "            cleaned_texts.append(tupled_files)\n",
    "            relative_sentence_index += 1\n",
    "    cleaned_corpus_as_dictionary['file_names'] = [x[0] for x in cleaned_texts]\n",
    "    cleaned_corpus_as_dictionary['sentences'] = [x[1] for x in cleaned_texts]\n",
    "    cleaned_corpus_as_dictionary['relative_sentence_index'] = [x[2] for x in cleaned_texts]\n",
    "    \n",
    "    df = pd.DataFrame(cleaned_corpus_as_dictionary)\n",
    "    print(\"\\n[-] Text preprocessing completed.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d3eba95-8f56-4c6e-8c0c-0c604e059a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_pos_counts_from_tagged_sentence(analyzed_sent):\n",
    "    pos_counts = []\n",
    "    pos_counts_raw = analyzed_sent.count_by(spacy.attrs.IDS['POS'])\n",
    "    for pos, count in pos_counts_raw.items():\n",
    "        tag = analyzed_sent.vocab[pos].text\n",
    "        pos_count = (tag, count)\n",
    "        pos_counts.append(pos_count)\n",
    "    # return a list of pos_counts\n",
    "    return pos_counts\n",
    "\n",
    "def pos_tag_sentence(sent):\n",
    "    tagged_sentence = []\n",
    "    analyzed_sent = nlp(sent, disable = ['ner'])\n",
    "    # getting the complete tokenized sentence\n",
    "    for token in analyzed_sent:\n",
    "        tagged_word = (token, token.pos_)\n",
    "        tagged_sentence.append(tagged_word)\n",
    "    pos_counts = get_pos_counts_from_tagged_sentence(analyzed_sent)\n",
    "    # return a tuple of both\n",
    "    return (tagged_sentence, pos_counts)\n",
    "\n",
    "def pos_tag_list_of_sentences(list_of_cleaned_sentences):\n",
    "    pos_tagged_text = []\n",
    "    for sent in list_of_cleaned_sentences:\n",
    "        tagged_sent = pos_tag_sentence(sent)\n",
    "        pos_tagged_text.append(tagged_sent)\n",
    "    # returns a list of tuples\n",
    "    return pos_tagged_text\n",
    "\n",
    "df strip_double_proper_nouns():\n",
    "    pass\n",
    "\n",
    "def pos_tag_texts_from_df(df, sentences_column='sentences'):\n",
    "    df['tagged_sentences'] = ''\n",
    "    df['pos_counts'] = ''\n",
    "    for index, row in df.iterrows():\n",
    "        progress(index, len(list(df.index.values)))\n",
    "        sentence = row[sentences_column]\n",
    "        tagged_sentence, pos_counts = pos_tag_sentence(sentence)\n",
    "        df.at[index, 'tagged_sentences'] = tagged_sentence\n",
    "        df.at[index, 'pos_counts'] = pos_counts\n",
    "        # print(tagged_sentence)\n",
    "        # if index >= 10:\n",
    "        #     break\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8426ac6a-be77-4590-a208-4079b96fb80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences_column = \"sentences\"\n",
    "# path_to_newspapers = \"../../../GitHub/ciphersofthetimes/data/corpora/newspapers_test/\"\n",
    "path_to_newspapers = \"../../data/corpora/newspapers_test/\"\n",
    "# path_to_spreadsheets = \"../../../GitHub/ciphersofthetimes/data/spreadsheets/\"\n",
    "path_to_spreadsheets = \"../../data/spreadsheets/\"\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     print(\"[+] Starting newspaper corpus processor...\")\n",
    "#     print(f\"[+] Using default values:\")\n",
    "#     print(f\"[+] Path to newspapers is: {path_to_newspapers}\")\n",
    "#     print(f\"[+] Path to spreadsheets is: {path_to_spreadsheets}\")\n",
    "#     print(f\"[+] Sentences column name is: {sentences_column}\")\n",
    "\n",
    "#     # path_to_newspapers = input(\"[+] Please input path to newspaper corpus: /Users/leehusigler/Documents/GitHub/ciphersofthetimes/data/corpora/newspapers_test\")\n",
    "#     print(f\"[+] Importing corpus of dirty texts from {path_to_newspapers}\")\n",
    "#     dirty_texts = input_corpus_of_txts(path=path_to_newspapers)\n",
    "#     print(\"[+] Processing dirty texts\")\n",
    "#     df = process_dirty_texts_to_df(dirty_texts)\n",
    "#     print(\"[+] Dataframe created.\")\n",
    "#     print(\"[+] Beginning POS tagging ...\")\n",
    "#     df = pos_tag_texts_from_df(df, 'sentences')\n",
    "#     print(\"[+] Completed POS tagging.\")\n",
    "#     print(\"[+] Dataframe head looks like this: \")\n",
    "#     print(df.head())\n",
    "#     print(\"[+] Saving dataframe...\")\n",
    "#     output_full(df=df, path_to_spreadsheets=path_to_spreadsheets)\n",
    "#     print(\"[+] Program completed. Exiting...\")\n",
    "    # exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394b9f7a-2bfe-420e-ba8b-eaaa8ab2e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[+] Starting newspaper corpus processor...\")\n",
    "print(f\"[+] Using default values:\")\n",
    "print(f\"[+] Path to newspapers is: {path_to_newspapers}\")\n",
    "print(f\"[+] Path to spreadsheets is: {path_to_spreadsheets}\")\n",
    "print(f\"[+] Sentences column name is: {sentences_column}\")\n",
    "\n",
    "# path_to_newspapers = input(\"[+] Please input path to newspaper corpus: /Users/leehusigler/Documents/GitHub/ciphersofthetimes/data/corpora/newspapers_test\")\n",
    "print(f\"[+] Importing corpus of dirty texts from {path_to_newspapers}\")\n",
    "dirty_texts = input_corpus_of_txts(path=path_to_newspapers)\n",
    "print(\"[+] Processing dirty texts\")\n",
    "df = process_dirty_texts_to_df(dirty_texts)\n",
    "print(\"[+] Dataframe created.\")\n",
    "print(\"[+] Beginning POS tagging ...\")\n",
    "df = pos_tag_texts_from_df(df, 'sentences')\n",
    "print(\"[+] Completed POS tagging.\")\n",
    "print(\"[+] Dataframe head looks like this: \")\n",
    "print(df.head())\n",
    "print(\"[+] Saving dataframe...\")\n",
    "output_full(df=df, path_to_spreadsheets=path_to_spreadsheets)\n",
    "print(\"[+] Program completed. Exiting...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866fc235-7df1-43f3-b54a-5be9f269d7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
