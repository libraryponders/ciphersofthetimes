{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51087402",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is the main jupyter project notebook for The Ciphers of <i>the Times</i>' computational analysis.\n",
    "### Within, we analyze two corpora. One is our custom curated corpus of \"Newspaper Novels,\" a genre of Sensation Novels from the 19th century\n",
    "### that emphasize their relationship to newspapers through content and style. Many of these novels weren coined \"Newspaper Novels\" by 19th century critics,\n",
    "### others were categorized as such by contemporary academics investigating the phenomenon of Agony Columns in the fiction novels of the Victorian era.\n",
    "### The other corpus we analyze here is from the NOVEL450 dataset, curated and analyzed by .txtLAB, a McGill-based research group: https://txtlab.org/\n",
    "### This corpus serves as a form of control corpus, a crutch to help identify particularities about the \"Newspaper Novel\" corpus. \n",
    "\n",
    "### We have attempted to organize this notebook in a way that would allow to run the same pipeline on both corpora with simplicity, clarity, and reproducability in mind.\n",
    "### But, we are by no means professional programmers, and this project has been an opportunity for the <i>the Times</i>' team members to learn and develop our skills\n",
    "### in Digital Humanities computational analysis. \n",
    "\n",
    "### We begin by importing the corpus from .txt files into a dataframe, and proceed to clean, extract metadata (basic statistics, TTR and MATTR, and POS tagging), \n",
    "### and save the new spreadsheets to perpare them for visualization and continued analysis. \n",
    "\n",
    "### The metadata outputs of this notebook already exists within the data/spreadsheets folder on github.\n",
    "### The full dataframes (with texts) can be reproduced by following the instructions below. We unfortunately could not include the dataframes containing\n",
    "### the full texts due to the space restriction on github for a single file.\n",
    "\n",
    "\n",
    "\n",
    "### jupyter lab --notebook-dir=E:/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b4630a5-d7ce-46ac-a3a9-29e17def8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE PATHS:\n",
    "### Paths are defined for the various spreadsheets and corpora used\n",
    "### Stopwords were initially taken from: https://towardsdatascience.com/getting-started-with-text-analysis-in-python-ca13590eb4f7\n",
    "### and then tailored to our pipeline\n",
    "\n",
    "## to Newpaper Novel corpus:\n",
    "path_to_nnovels_corpus = '../../data/corpora/corpus_newspaper_novels/' \n",
    "\n",
    "## to assets:\n",
    "path_to_assets = '../../assets/'\n",
    "\n",
    "## to all spreadsheets:\n",
    "path_to_spreadsheets = '../../data/spreadsheets/'\n",
    "\n",
    "## to .txtLab corpus consisting of 150 English-language novels:\n",
    "# path_to_dirty_txtlab_corpus = '../../data/spreadsheets/dirtyengnovels-211215.csv' ## This no longer exists, left for pipeline viewing. See df_txtlab_meta.csv for results. \n",
    "\n",
    "## nnovels corpus metadata:\n",
    "nnovels_corpus_metadata = '../../data/spreadsheets/nnovels_corpus_metadata.csv' \n",
    "\n",
    "## stop words:\n",
    "stopwords_file = 'stopwords.txt'\n",
    "\n",
    "## characters and numbers to exclude from texts:\n",
    "exclude_file = 'characters_and_numbers_to_exclude.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83095e22-84b3-4f8b-a47f-588edea343b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports, utility, and important functions begin here:\n",
    "\n",
    "## basic libraries:\n",
    "import os\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "# lexical diversity library:\n",
    "from lexicalrichness import LexicalRichness\n",
    "\n",
    "# to be able to see more columns when dataframes are printed out:\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# to not get copy warnings when splitting dataframes\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6f42bc8-6955-44c2-805b-0e63bed94e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STOPWORDS and elements to exclude\n",
    "# opening stopwords and characters to exclude from assets\n",
    "with codecs.open(path_to_assets + stopwords_file, 'r', encoding='utf-8', errors=\"ignore\") as stopwords_raw:\n",
    "    stopwords = stopwords_raw.read()\n",
    "    stopwords = stopwords.split()\n",
    "with codecs.open(path_to_assets + exclude_file, 'r', encoding='utf-8', errors=\"ignore\") as characters_to_exclude_raw:\n",
    "    characters_to_exclude = characters_to_exclude_raw.read()\n",
    "    characters_to_exclude = characters_to_exclude.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78bc2971-ff32-48b9-8fa2-82b01e1b0412",
   "metadata": {},
   "outputs": [],
   "source": [
    "### UTILITY CODE:\n",
    "\n",
    "## This section allows us to open large dataframes by redefining the max size of the csv fields.\n",
    "## Solution taken from: https://stackoverflow.com/questions/15063936/csv-error-field-larger-than-field-limit-131072\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10 as long as the OverflowError occurs\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7194395f-b662-4503-8ef4-5f9c4edd5595",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS 1/6\n",
    "### Various functions for the project begin here \n",
    "\n",
    "## Progress bar to view the progress of lengthy processes\n",
    "# As suggested by Rom Ruben (see: http://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console/27871113#comment50529068_27871113)\n",
    "def progress(count, total, status=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "    percents = round(100.1 * count / float(total), 1)\n",
    "    bar = '#' * filled_len + '-' * (bar_len - filled_len)\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n",
    "    sys.stdout.flush()  \n",
    "    \n",
    "\n",
    "## Used to incorperate a metadata spreadsheet and gather an already ordered corpus within a file,\n",
    "## and returns a dataframe \n",
    "def import_corpus_and_meta(path_to_corpus=path_to_nnovels_corpus, path_to_meta_data=nnovels_corpus_metadata):\n",
    "    # read in metadata\n",
    "    df = pd.read_csv(path_to_meta_data, engine='python')\n",
    "    \n",
    "    # drop faulty index\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    \n",
    "    # setting up text_index to ensure sequentiallity\n",
    "    text_index = 0\n",
    "    \n",
    "    # grab all texts from corpus and strip of project gutenberg endtext\n",
    "    #TODO: THIS PROCESS NEEDS TO BE COMPLETED MANUALLY ON CORPUS AND DELETED\n",
    "    split_on = [\"END OF THE PROJECT GUTENBERG\",\"End of the Project Gutenberg EBook\",\"End of Project Gutenberg\",\"End of The Project Gutenberg\"] \n",
    "    \n",
    "    # loop through each novel in the directory, open the file\n",
    "    for textname in os.listdir(path_to_nnovels_corpus):\n",
    "        with codecs.open(path_to_nnovels_corpus + textname, 'r', encoding='utf-8', errors=\"ignore\") as raw_text:\n",
    "            dirty_text = raw_text.read()\n",
    "            \n",
    "            # getting rid of the project gutenberg endtext\n",
    "            for text in split_on:\n",
    "                dirty_text = dirty_text.split(text)[0]\n",
    "                \n",
    "            # input into df\n",
    "            df.at[text_index, 'dirty_text'] = dirty_text\n",
    "            text_index += 1\n",
    "        # show progress\n",
    "        progress(text_index, len(os.listdir(path_to_corpus)))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6611189a-65e7-48c6-81c4-e371cf6c91f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS 2/6\n",
    "\n",
    "## Cleans a text string and returns the cleaned text, \n",
    "## the text without stopwords filtered (string), the text with stopwords filtered (list), and the text as sentences (list)\n",
    "def clean_text(text, stopwords=stopwords, characters_to_exclude=characters_to_exclude):\n",
    "    # lowercasing the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # removing all characters in characters_to_exclude\n",
    "    text = ''.join(char for char in text if char not in characters_to_exclude)\n",
    "    \n",
    "    # replacing all newline '\\n' with spaces\n",
    "    text = text.replace('\\n', \" \")\n",
    "    \n",
    "    # replacing all multiple spaces with a single space\n",
    "    text = re.sub('\\s+',' ', text)\n",
    "    \n",
    "    # getting list of sentences\n",
    "    text_split_sentences = re.split(r\"\\.|\\:|\\?|\\!\", text)\n",
    "    \n",
    "    # getting rid of empty elements in sentences\n",
    "    text_split_sentences = list(filter(None, text_split_sentences))\n",
    "    \n",
    "    # splitting text by spaces for tokenization\n",
    "    text_split = re.split(r\"\\s\", text)\n",
    "    \n",
    "    # removing all empty elements in text_split\n",
    "    text_split_stopless = list(filter(None, text_split))\n",
    "    \n",
    "    # getting rid of end-of-line punctuation:\n",
    "    text_split_stopless = [word.strip(\".?:!\") for word in text_split_stopless]\n",
    "    \n",
    "    # getting rid of all stopwords:\n",
    "    text_split_stopped = [word for word in text_split_stopless if word not in stopwords]\n",
    "\n",
    "    return text, text_split_stopless, text_split_stopped, text_split_sentences\n",
    "\n",
    "\n",
    "## Takes a dataframe, a dirty_text column, a characters_to_exclude file, and a stopwords file and returns the clean text, inputting it into the df\n",
    "def clean_up_corpus_and_grab_basic_stats(df, dirty_text_column='dirty_text'):\n",
    "    print(\"Cleaning text, assigning them to columns, and grabbing basic stats...\")\n",
    "    \n",
    "    # creating columns for data\n",
    "    df['words_standardized_stopped'] = ''\n",
    "    df['sentences_count'] = 0\n",
    "    df['average_words_per_sentence'] = 0.0\n",
    "    df['sentences_standardized_stopless'] = ''\n",
    "    \n",
    "    # loop through the dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        # get the clean text for each novel\n",
    "        text, text_split_stopless, text_split_stopped, text_split_sentences = clean_text(row[dirty_text_column])\n",
    "\n",
    "        # getting basic stats for tokenized texts (words):\n",
    "        words_count_stopless = len(text_split_stopless)\n",
    "        words_count_stopped = len(text_split_stopped)\n",
    "        percentage_stopped_of_stoppless = (words_count_stopped / words_count_stopless) * 100\n",
    "\n",
    "        # getting basic stats for tokenized texts (sentences):\n",
    "        sentences_count = len(text_split_sentences)\n",
    "        words_per_sentence = [len(sentence.split()) for sentence in text_split_sentences]\n",
    "        total = sum(words_per_sentence)\n",
    "        average_words_per_sentence = int(total) / len(words_per_sentence)\n",
    "        \n",
    "        # inputting data into df\n",
    "        df.at[index, 'words_as_string_for_vectorizor'] = text\n",
    "        df.at[index, 'words_count_stopless'] = words_count_stopless\n",
    "        df.at[index, 'words_count_stopped'] = words_count_stopped\n",
    "        df.at[index, 'words_standardized_stopped'] = text_split_stopped\n",
    "        df.at[index, 'percentage_stopped_of_stoppless'] = percentage_stopped_of_stoppless\n",
    "        df.at[index, 'sentences_standardized_stopless'] = text_split_sentences\n",
    "        df.at[index, 'sentences_count'] = sentences_count\n",
    "        df.at[index, 'average_words_per_sentence'] = average_words_per_sentence\n",
    "\n",
    "        # show progress bar\n",
    "        progress(index, len(df.index))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c06f55ce-4001-4798-aa4c-0116837d84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS 3/6\n",
    "\n",
    "## Takes a dataframe, a text column (string), and several parameters, and returns the ttr and mattr (with multiple configurations) for each row:\n",
    "def run_ttr_analysis_on_df(df, text_column='words_as_string_for_vectorizor', full_text_ttr=True, moving_average_ttr=True, window_sizes=[500, 2000]):\n",
    "    for index, row in df.iterrows():\n",
    "        # grabs text column from df\n",
    "        text = row[text_column]\n",
    "        # gets lex from lexical richness library\n",
    "        lex = LexicalRichness(text)\n",
    "        # these switches are here in case someone wants to run just ttr/mattr (since this can take some time)\n",
    "        if full_text_ttr == True:\n",
    "            ttr = lex.ttr\n",
    "            df.loc[index, 'full_text_ttr'] = ttr\n",
    "        if moving_average_ttr == True:\n",
    "            for window_size in window_sizes:\n",
    "                if (window_size != None) and (len(text) > window_size):\n",
    "                    mattr = lex.mattr(window_size=window_size)\n",
    "                    df.loc[index, f'mattr_{str(window_size)}'] = mattr\n",
    "                    \n",
    "        # show progress bar\n",
    "        progress(index, len(df.index))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1acfd123-be6b-4388-8a03-7f4abda47a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS 4/6\n",
    "\n",
    "## Takes in a dataframe, a list of columns to visualize, and a date_column, then visualizes it over time using the matplotlib.\n",
    "## Created as a slightly simpler way to check and compare visualizations between different columns and/or dataframes\n",
    "\n",
    "#TODO: make multiple dfs visualizable:\n",
    "def visualize_numerical_columns__over_time(df, list_of_columns_to_visualize, date_column='book_year', graph_y_label='What are we counting?', title='SOMETHING over Time'):\n",
    "    # importing libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "    \n",
    "    # setting up lists to capture the years, counts, labels, and colours to visualize\n",
    "    years_list = []\n",
    "    counts_list = []\n",
    "    labels_list = []\n",
    "    colors_list = []\n",
    "    for column_name in list_of_columns_to_visualize:\n",
    "        \n",
    "        # grouping the column by years and getting arrays for graph\n",
    "        grouped_by_year = pd.to_numeric(df[column_name]).groupby(df[date_column])\n",
    "        grouped_by_year = grouped_by_year.mean().reset_index()\n",
    "        years = np.array(grouped_by_year[date_column].tolist())\n",
    "        count_to_visualize = np.array(grouped_by_year[column_name].tolist())\n",
    "        \n",
    "        # saving the info for each column to visualize in a list\n",
    "        years_list.append(years)\n",
    "        counts_list.append(count_to_visualize)\n",
    "        labels_list.append(column_name)\n",
    "        \n",
    "        # getting random colors to differenciate between visualized data\n",
    "        r = random.random()\n",
    "        b = random.random()\n",
    "        g = random.random()\n",
    "        color = (r, g, b)\n",
    "        colors_list.append(color)\n",
    "        \n",
    "    # plotting SOMETHING over time:\n",
    "    plt.figure(figsize=(20,10))\n",
    "    for index in range(len(labels_list)):\n",
    "        plt.plot(years_list[index], counts_list[index], label=labels_list[index], c=colors_list[index])\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ea7b377-33ad-4eaa-aa00-552c0bae7b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS 5/6\n",
    "## POS tagging:\n",
    "\n",
    "# first loading english language support\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "## Takes in a dataframe and clean text column (as string), and returns the df with POS tags for all the texts\n",
    "## Multiple columns are created, one for each POS tag, and one that contains all POS tags (I did this to more easily\n",
    "## be able to grab POS percentages afterward)\n",
    "def get_POS_tags_for_text_in_df(df, text_row_to_analyze='words_as_string_for_vectorizor'):\n",
    "    \n",
    "    # setting up column for pos counts\n",
    "    df['all_pos_counts'] = ''\n",
    "    \n",
    "    # loop through df and get all POS tags:\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        # grab text\n",
    "        text = row[text_row_to_analyze]\n",
    "        \n",
    "        # this is a memory buffer, to extend max length of available ram according to the text being analyzed\n",
    "        # https://datascience.stackexchange.com/questions/38745/increasing-spacy-max-nlp-limit\n",
    "        nlp.max_length = len(text) + 100\n",
    "        \n",
    "        # disable modules not in use to save memory\n",
    "        analyzed_doc = nlp(text, disable = ['ner'])\n",
    "        \n",
    "        # grabbing all pos counts in the text in non-human readable format\n",
    "        pos_counts_in_text = analyzed_doc.count_by(spacy.attrs.IDS['POS'])\n",
    "        \n",
    "        # setting up list to render pos hashes in human readable format:\n",
    "        human_readable_pos_count_list = []\n",
    "        \n",
    "        # iterating through counts to make hashes human readable:\n",
    "        for pos, count in pos_counts_in_text.items():\n",
    "            human_readable_tag = analyzed_doc.vocab[pos].text\n",
    "            # rendering as list to input back into df\n",
    "            human_readable_tag_and_count = list((human_readable_tag, count))\n",
    "            human_readable_pos_count_list.append(human_readable_tag_and_count)\n",
    "        \n",
    "        # looping through the human readable counts, assigning their label to the column\n",
    "        # and the count to the row for each pos tag\n",
    "        for element in human_readable_pos_count_list:\n",
    "            df.at[index, 'POS_' + str(element[0])+'_count'] = element[1]\n",
    "        \n",
    "        # placing all the pos counts for each text in the all_pos_counts column\n",
    "        df.at[index, 'all_pos_counts'] = human_readable_pos_count_list\n",
    "        \n",
    "        # show progress\n",
    "        progress(index, len(df.index))\n",
    "        \n",
    "    # getting POS percentages for each POS tag in texts\n",
    "    # There are much easier and more efficient ways to do this rather than looping over the entire df again but we were pressed for time...\n",
    "    # TODO: integrate this loop into previous loop\n",
    "    for index, row in df.iterrows():\n",
    "        total = 0.0\n",
    "        for name in df.columns.values.tolist():\n",
    "            if name.startswith(\"POS_\"):\n",
    "                # get total POS elements count for sanity\n",
    "                total += row[name]\n",
    "        df.at[index, \"parts_of_speech_total_count\"] = int(total)\n",
    "        \n",
    "        for name in df.columns.values.tolist():\n",
    "            if name.startswith(\"POS_\"):\n",
    "                # assign new name for column\n",
    "                new_name = \"%\" + name\n",
    "                # get % of total POS in text\n",
    "                percentage = round((row[name] / total) * float(100), 3)\n",
    "                # if this is the first index, create the column name to avoid errors\n",
    "                if index == 0:\n",
    "                    df[new_name] = 0.0\n",
    "                df.at[index, new_name] = percentage\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "992bf5d2-2284-4c2a-a172-3692797834c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS 6/6\n",
    "\n",
    "## Functions that 1) save just metadata, 2) save the full dataframe, 3) load the dataframe\n",
    "\n",
    "# for metadata output:\n",
    "def output_metadata(df, spreadsheet_name='FILL_IN_SPREADSHEET_NAME_META', path_to_spreadsheets=path_to_spreadsheets):\n",
    "    # setting up list of columns NOT to export (since this is just metadata)\n",
    "    list_of_columns_not_to_include = ['words_standardized_stopped', 'sentences_standardized_stopless','words_as_string_for_vectorizor', 'dirty_text']\n",
    "    # all other columns are included\n",
    "    columns_to_include = [column_name for column_name in df.columns.values.tolist() if column_name.lower() not in list_of_columns_not_to_include]\n",
    "    df_meta = df[columns_to_include]\n",
    "    df_meta.to_csv(path_to_spreadsheets + spreadsheet_name + '.csv')\n",
    "    print(spreadsheet_name + ' was saved in '+str(path_to_spreadsheets))\n",
    "    \n",
    "# for full output:\n",
    "def output_full(df, spreadsheet_name='FILL_IN_SPREADSHEET_NAME_FULL', path_to_spreadsheets=path_to_spreadsheets):\n",
    "    df.to_csv(path_to_spreadsheets + spreadsheet_name + '.csv')\n",
    "    print(spreadsheet_name + ' was saved in '+str(path_to_spreadsheets))\n",
    "\n",
    "# load a dataframe\n",
    "def open_df_and_print(file_name='df_full.csv', path_to_spreadsheets=path_to_spreadsheets, drop_first_column=False):\n",
    "    df = pd.read_csv(path_to_spreadsheets + file_name, engine='python')\n",
    "    if drop_first_column == True:\n",
    "        df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    return df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f66625f-c2ae-4945-8967-e1dfae342a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c2d6f8e-b845-4f6f-890c-e2b863ee693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN UP TO HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b452736-e76f-4304-9e64-cdb2638fdbde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a55fb6-c196-400b-b16d-f1b331a62bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2add44-412e-4915-88e4-4ad9f70383f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba24ada6-9897-4ab1-aafd-a45a4ad85676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503f9c51-3592-4f1e-89a8-1af8479051bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f34f759-1194-4883-a930-85c01cd7db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## BEGINNING OF NNOVEL ANALYSIS:\n",
    "# # ## IF THIS NEEDS TO BE RUN, UNCOMMENT EVERYTHING AND RUN (make sure that the paths at the top of the notebook correspond with your paths)\n",
    "\n",
    "# # Complete Initial Pipeline (needs to be run once):\n",
    "# print(\"Starting pipeline...\")\n",
    "\n",
    "# print(\"\\nImporting corpus from \"+ str(path_to_nnovels_corpus)+ \" and metadata from \"+ str(nnovels_corpus_metadata))\n",
    "# df = import_corpus_and_meta(path_to_nnovels_corpus, nnovels_corpus_metadata)\n",
    "\n",
    "# # for testing purposes: \n",
    "# # df = df_all.loc[:5,:]\n",
    "\n",
    "# print('\\nCleaning up corpus and grabbing basic statistics')\n",
    "# df = clean_up_corpus_and_grab_basic_stats(df)\n",
    "# df.drop('dirty_text', axis=1, inplace=True)\n",
    "\n",
    "# print('\\nRunning TTR analysis (this can take some time)')\n",
    "# df = run_ttr_analysis_on_df(df)\n",
    "\n",
    "# print('\\nRunning POS analysis (this can take some time)')\n",
    "# df = get_POS_tags_for_text_in_df(df, text_row_to_analyze='words_as_string_for_vectorizor')\n",
    "\n",
    "# df.head()\n",
    "# print('\\nSaving df')\n",
    "# output_metadata(df, spreadsheet_name = 'df_meta')\n",
    "# output_full(df, spreadsheet_name = 'df_full')\n",
    "# print('\\nPrinting df')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc40629c-79db-4a1d-81aa-eb013d9916ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e62061b3-55b1-4ad1-a6d1-e10d5cf379e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### If initial pipeline has already run:\n",
    "### Import the dfs from spreadsheets folder \n",
    "\n",
    "# df_nnovels_full = open_df_and_print(file_name = 'df_nnovels_full.csv', path_to_spreadsheets=path_to_spreadsheets, drop_first_column=True)\n",
    "# df_nnovels_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d9da087-682a-4b15-b591-dd5fcbdaec2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_title</th>\n",
       "      <th>book_year</th>\n",
       "      <th>author_name</th>\n",
       "      <th>words_count_stopless</th>\n",
       "      <th>words_count_stopped</th>\n",
       "      <th>percentage_stopped_of_stoppless</th>\n",
       "      <th>sentences_count</th>\n",
       "      <th>average_words_per_sentence</th>\n",
       "      <th>full_text_ttr</th>\n",
       "      <th>mattr_500</th>\n",
       "      <th>mattr_2000</th>\n",
       "      <th>all_pos_counts</th>\n",
       "      <th>50_most_common_nouns</th>\n",
       "      <th>50_most_common_verbs</th>\n",
       "      <th>POS_PUNCT</th>\n",
       "      <th>POS_NOUN</th>\n",
       "      <th>POS_DET</th>\n",
       "      <th>POS_ADJ</th>\n",
       "      <th>POS_VERB</th>\n",
       "      <th>POS_SCONJ</th>\n",
       "      <th>POS_ADP</th>\n",
       "      <th>POS_AUX</th>\n",
       "      <th>POS_PART</th>\n",
       "      <th>POS_CCONJ</th>\n",
       "      <th>POS_ADV</th>\n",
       "      <th>POS_PROPN</th>\n",
       "      <th>POS_PRON</th>\n",
       "      <th>POS_NUM</th>\n",
       "      <th>POS_INTJ</th>\n",
       "      <th>POS_X</th>\n",
       "      <th>noun_percentage</th>\n",
       "      <th>verb_and_aux_percentage</th>\n",
       "      <th>POS_SYM</th>\n",
       "      <th>parts_of_speech_total_count</th>\n",
       "      <th>%POS_PUNCT</th>\n",
       "      <th>%POS_NOUN</th>\n",
       "      <th>%POS_DET</th>\n",
       "      <th>%POS_ADJ</th>\n",
       "      <th>%POS_VERB</th>\n",
       "      <th>%POS_SCONJ</th>\n",
       "      <th>%POS_ADP</th>\n",
       "      <th>%POS_AUX</th>\n",
       "      <th>%POS_PART</th>\n",
       "      <th>%POS_CCONJ</th>\n",
       "      <th>%POS_ADV</th>\n",
       "      <th>%POS_PROPN</th>\n",
       "      <th>%POS_PRON</th>\n",
       "      <th>%POS_NUM</th>\n",
       "      <th>%POS_INTJ</th>\n",
       "      <th>%POS_X</th>\n",
       "      <th>%POS_SYM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agnes_Grey</td>\n",
       "      <td>1847</td>\n",
       "      <td>Anne_Bronte</td>\n",
       "      <td>67718.0</td>\n",
       "      <td>24161.0</td>\n",
       "      <td>35.678845</td>\n",
       "      <td>3201.0</td>\n",
       "      <td>21.175258</td>\n",
       "      <td>0.108841</td>\n",
       "      <td>0.519898</td>\n",
       "      <td>0.368708</td>\n",
       "      <td>[['PUNCT', 4930], ['NOUN', 10116], ['DET', 825...</td>\n",
       "      <td>[('time', 142), ('mother', 124), ('day', 65), ...</td>\n",
       "      <td>[('would', 379), ('could', 260), ('said', 186)...</td>\n",
       "      <td>4930.0</td>\n",
       "      <td>10116.0</td>\n",
       "      <td>8255.0</td>\n",
       "      <td>4864.0</td>\n",
       "      <td>10808.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>6730.0</td>\n",
       "      <td>3776.0</td>\n",
       "      <td>2632.0</td>\n",
       "      <td>3931.0</td>\n",
       "      <td>5084.0</td>\n",
       "      <td>2033.0</td>\n",
       "      <td>8228.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>13.660</td>\n",
       "      <td>14.594</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74058.0</td>\n",
       "      <td>6.657</td>\n",
       "      <td>13.660</td>\n",
       "      <td>11.147</td>\n",
       "      <td>6.568</td>\n",
       "      <td>14.594</td>\n",
       "      <td>2.701</td>\n",
       "      <td>9.087</td>\n",
       "      <td>5.099</td>\n",
       "      <td>3.554</td>\n",
       "      <td>5.308</td>\n",
       "      <td>6.865</td>\n",
       "      <td>2.745</td>\n",
       "      <td>11.110</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Armadale</td>\n",
       "      <td>1864</td>\n",
       "      <td>Wilkie_Collins</td>\n",
       "      <td>295546.0</td>\n",
       "      <td>111234.0</td>\n",
       "      <td>37.636781</td>\n",
       "      <td>19298.0</td>\n",
       "      <td>15.344647</td>\n",
       "      <td>0.045656</td>\n",
       "      <td>0.497521</td>\n",
       "      <td>0.341371</td>\n",
       "      <td>[['ADP', 36354], ['PUNCT', 20620], ['DET', 436...</td>\n",
       "      <td>[('time', 864), ('midwinter', 786), ('man', 70...</td>\n",
       "      <td>[('said', 1136), ('will', 694), ('would', 578)...</td>\n",
       "      <td>20620.0</td>\n",
       "      <td>52174.0</td>\n",
       "      <td>43605.0</td>\n",
       "      <td>18284.0</td>\n",
       "      <td>44704.0</td>\n",
       "      <td>6609.0</td>\n",
       "      <td>36354.0</td>\n",
       "      <td>16230.0</td>\n",
       "      <td>9975.0</td>\n",
       "      <td>9477.0</td>\n",
       "      <td>17419.0</td>\n",
       "      <td>12346.0</td>\n",
       "      <td>32389.0</td>\n",
       "      <td>1712.0</td>\n",
       "      <td>541.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>16.180</td>\n",
       "      <td>13.863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>322464.0</td>\n",
       "      <td>6.395</td>\n",
       "      <td>16.180</td>\n",
       "      <td>13.522</td>\n",
       "      <td>5.670</td>\n",
       "      <td>13.863</td>\n",
       "      <td>2.050</td>\n",
       "      <td>11.274</td>\n",
       "      <td>5.033</td>\n",
       "      <td>3.093</td>\n",
       "      <td>2.939</td>\n",
       "      <td>5.402</td>\n",
       "      <td>3.829</td>\n",
       "      <td>10.044</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aurora_Floyd</td>\n",
       "      <td>1863</td>\n",
       "      <td>Mary_Elizabeth_Braddon</td>\n",
       "      <td>183016.0</td>\n",
       "      <td>74878.0</td>\n",
       "      <td>40.913363</td>\n",
       "      <td>9425.0</td>\n",
       "      <td>19.422599</td>\n",
       "      <td>0.075907</td>\n",
       "      <td>0.529468</td>\n",
       "      <td>0.381519</td>\n",
       "      <td>[['PROPN', 9662], ['ADV', 10206], ['DET', 2736...</td>\n",
       "      <td>[('man', 506), ('face', 245), ('time', 233), (...</td>\n",
       "      <td>[('said', 562), ('would', 553), ('could', 449)...</td>\n",
       "      <td>10576.0</td>\n",
       "      <td>33566.0</td>\n",
       "      <td>27368.0</td>\n",
       "      <td>13795.0</td>\n",
       "      <td>25284.0</td>\n",
       "      <td>5435.0</td>\n",
       "      <td>20867.0</td>\n",
       "      <td>9771.0</td>\n",
       "      <td>4237.0</td>\n",
       "      <td>7753.0</td>\n",
       "      <td>10206.0</td>\n",
       "      <td>9662.0</td>\n",
       "      <td>15633.0</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>17.151</td>\n",
       "      <td>12.919</td>\n",
       "      <td>2.0</td>\n",
       "      <td>195706.0</td>\n",
       "      <td>5.404</td>\n",
       "      <td>17.151</td>\n",
       "      <td>13.984</td>\n",
       "      <td>7.049</td>\n",
       "      <td>12.919</td>\n",
       "      <td>2.777</td>\n",
       "      <td>10.662</td>\n",
       "      <td>4.993</td>\n",
       "      <td>2.165</td>\n",
       "      <td>3.962</td>\n",
       "      <td>5.215</td>\n",
       "      <td>4.937</td>\n",
       "      <td>7.988</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Belinda:A_Novel</td>\n",
       "      <td>1883</td>\n",
       "      <td>Rhoda_Broughton</td>\n",
       "      <td>147937.0</td>\n",
       "      <td>57386.0</td>\n",
       "      <td>38.790837</td>\n",
       "      <td>10248.0</td>\n",
       "      <td>14.100898</td>\n",
       "      <td>0.092824</td>\n",
       "      <td>0.534271</td>\n",
       "      <td>0.384540</td>\n",
       "      <td>[['PROPN', 5341], ['ADP', 14886], ['DET', 1801...</td>\n",
       "      <td>[('eyes', 227), ('voice', 212), ('one', 175), ...</td>\n",
       "      <td>[('says', 577), ('will', 562), ('would', 431),...</td>\n",
       "      <td>11552.0</td>\n",
       "      <td>23275.0</td>\n",
       "      <td>18018.0</td>\n",
       "      <td>10336.0</td>\n",
       "      <td>20488.0</td>\n",
       "      <td>4025.0</td>\n",
       "      <td>14886.0</td>\n",
       "      <td>9313.0</td>\n",
       "      <td>4134.0</td>\n",
       "      <td>4949.0</td>\n",
       "      <td>10799.0</td>\n",
       "      <td>5341.0</td>\n",
       "      <td>15702.0</td>\n",
       "      <td>908.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>15.085</td>\n",
       "      <td>13.278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>154295.0</td>\n",
       "      <td>7.487</td>\n",
       "      <td>15.085</td>\n",
       "      <td>11.678</td>\n",
       "      <td>6.699</td>\n",
       "      <td>13.278</td>\n",
       "      <td>2.609</td>\n",
       "      <td>9.648</td>\n",
       "      <td>6.036</td>\n",
       "      <td>2.679</td>\n",
       "      <td>3.207</td>\n",
       "      <td>6.999</td>\n",
       "      <td>3.462</td>\n",
       "      <td>10.177</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Birds_of_Prey</td>\n",
       "      <td>1867</td>\n",
       "      <td>Mary_Elizabeth_Braddon</td>\n",
       "      <td>161547.0</td>\n",
       "      <td>64208.0</td>\n",
       "      <td>39.745709</td>\n",
       "      <td>8357.0</td>\n",
       "      <td>19.335168</td>\n",
       "      <td>0.083881</td>\n",
       "      <td>0.526442</td>\n",
       "      <td>0.376160</td>\n",
       "      <td>[['PROPN', 8773], ['DET', 23096], ['ADP', 1857...</td>\n",
       "      <td>[('man', 426), ('time', 269), ('life', 233), (...</td>\n",
       "      <td>[('would', 490), ('said', 422), ('could', 327)...</td>\n",
       "      <td>9019.0</td>\n",
       "      <td>29719.0</td>\n",
       "      <td>23096.0</td>\n",
       "      <td>12741.0</td>\n",
       "      <td>21333.0</td>\n",
       "      <td>4212.0</td>\n",
       "      <td>18573.0</td>\n",
       "      <td>9380.0</td>\n",
       "      <td>4281.0</td>\n",
       "      <td>6808.0</td>\n",
       "      <td>8960.0</td>\n",
       "      <td>8773.0</td>\n",
       "      <td>13952.0</td>\n",
       "      <td>1085.0</td>\n",
       "      <td>427.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>17.238</td>\n",
       "      <td>12.374</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172405.0</td>\n",
       "      <td>5.231</td>\n",
       "      <td>17.238</td>\n",
       "      <td>13.396</td>\n",
       "      <td>7.390</td>\n",
       "      <td>12.374</td>\n",
       "      <td>2.443</td>\n",
       "      <td>10.773</td>\n",
       "      <td>5.441</td>\n",
       "      <td>2.483</td>\n",
       "      <td>3.949</td>\n",
       "      <td>5.197</td>\n",
       "      <td>5.089</td>\n",
       "      <td>8.093</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        book_title  book_year             author_name  words_count_stopless  \\\n",
       "0       Agnes_Grey       1847             Anne_Bronte               67718.0   \n",
       "1         Armadale       1864          Wilkie_Collins              295546.0   \n",
       "2     Aurora_Floyd       1863  Mary_Elizabeth_Braddon              183016.0   \n",
       "3  Belinda:A_Novel       1883         Rhoda_Broughton              147937.0   \n",
       "4    Birds_of_Prey       1867  Mary_Elizabeth_Braddon              161547.0   \n",
       "\n",
       "   words_count_stopped  percentage_stopped_of_stoppless  sentences_count  \\\n",
       "0              24161.0                        35.678845           3201.0   \n",
       "1             111234.0                        37.636781          19298.0   \n",
       "2              74878.0                        40.913363           9425.0   \n",
       "3              57386.0                        38.790837          10248.0   \n",
       "4              64208.0                        39.745709           8357.0   \n",
       "\n",
       "   average_words_per_sentence  full_text_ttr  mattr_500  mattr_2000  \\\n",
       "0                   21.175258       0.108841   0.519898    0.368708   \n",
       "1                   15.344647       0.045656   0.497521    0.341371   \n",
       "2                   19.422599       0.075907   0.529468    0.381519   \n",
       "3                   14.100898       0.092824   0.534271    0.384540   \n",
       "4                   19.335168       0.083881   0.526442    0.376160   \n",
       "\n",
       "                                      all_pos_counts  \\\n",
       "0  [['PUNCT', 4930], ['NOUN', 10116], ['DET', 825...   \n",
       "1  [['ADP', 36354], ['PUNCT', 20620], ['DET', 436...   \n",
       "2  [['PROPN', 9662], ['ADV', 10206], ['DET', 2736...   \n",
       "3  [['PROPN', 5341], ['ADP', 14886], ['DET', 1801...   \n",
       "4  [['PROPN', 8773], ['DET', 23096], ['ADP', 1857...   \n",
       "\n",
       "                                50_most_common_nouns  \\\n",
       "0  [('time', 142), ('mother', 124), ('day', 65), ...   \n",
       "1  [('time', 864), ('midwinter', 786), ('man', 70...   \n",
       "2  [('man', 506), ('face', 245), ('time', 233), (...   \n",
       "3  [('eyes', 227), ('voice', 212), ('one', 175), ...   \n",
       "4  [('man', 426), ('time', 269), ('life', 233), (...   \n",
       "\n",
       "                                50_most_common_verbs  POS_PUNCT  POS_NOUN  \\\n",
       "0  [('would', 379), ('could', 260), ('said', 186)...     4930.0   10116.0   \n",
       "1  [('said', 1136), ('will', 694), ('would', 578)...    20620.0   52174.0   \n",
       "2  [('said', 562), ('would', 553), ('could', 449)...    10576.0   33566.0   \n",
       "3  [('says', 577), ('will', 562), ('would', 431),...    11552.0   23275.0   \n",
       "4  [('would', 490), ('said', 422), ('could', 327)...     9019.0   29719.0   \n",
       "\n",
       "   POS_DET  POS_ADJ  POS_VERB  POS_SCONJ  POS_ADP  POS_AUX  POS_PART  \\\n",
       "0   8255.0   4864.0   10808.0     2000.0   6730.0   3776.0    2632.0   \n",
       "1  43605.0  18284.0   44704.0     6609.0  36354.0  16230.0    9975.0   \n",
       "2  27368.0  13795.0   25284.0     5435.0  20867.0   9771.0    4237.0   \n",
       "3  18018.0  10336.0   20488.0     4025.0  14886.0   9313.0    4134.0   \n",
       "4  23096.0  12741.0   21333.0     4212.0  18573.0   9380.0    4281.0   \n",
       "\n",
       "   POS_CCONJ  POS_ADV  POS_PROPN  POS_PRON  POS_NUM  POS_INTJ  POS_X  \\\n",
       "0     3931.0   5084.0     2033.0    8228.0    357.0     277.0   37.0   \n",
       "1     9477.0  17419.0    12346.0   32389.0   1712.0     541.0   25.0   \n",
       "2     7753.0  10206.0     9662.0   15633.0   1101.0     394.0   56.0   \n",
       "3     4949.0  10799.0     5341.0   15702.0    908.0     444.0  125.0   \n",
       "4     6808.0   8960.0     8773.0   13952.0   1085.0     427.0   46.0   \n",
       "\n",
       "   noun_percentage  verb_and_aux_percentage  POS_SYM  \\\n",
       "0           13.660                   14.594      0.0   \n",
       "1           16.180                   13.863      0.0   \n",
       "2           17.151                   12.919      2.0   \n",
       "3           15.085                   13.278      0.0   \n",
       "4           17.238                   12.374      0.0   \n",
       "\n",
       "   parts_of_speech_total_count  %POS_PUNCT  %POS_NOUN  %POS_DET  %POS_ADJ  \\\n",
       "0                      74058.0       6.657     13.660    11.147     6.568   \n",
       "1                     322464.0       6.395     16.180    13.522     5.670   \n",
       "2                     195706.0       5.404     17.151    13.984     7.049   \n",
       "3                     154295.0       7.487     15.085    11.678     6.699   \n",
       "4                     172405.0       5.231     17.238    13.396     7.390   \n",
       "\n",
       "   %POS_VERB  %POS_SCONJ  %POS_ADP  %POS_AUX  %POS_PART  %POS_CCONJ  %POS_ADV  \\\n",
       "0     14.594       2.701     9.087     5.099      3.554       5.308     6.865   \n",
       "1     13.863       2.050    11.274     5.033      3.093       2.939     5.402   \n",
       "2     12.919       2.777    10.662     4.993      2.165       3.962     5.215   \n",
       "3     13.278       2.609     9.648     6.036      2.679       3.207     6.999   \n",
       "4     12.374       2.443    10.773     5.441      2.483       3.949     5.197   \n",
       "\n",
       "   %POS_PROPN  %POS_PRON  %POS_NUM  %POS_INTJ  %POS_X  %POS_SYM  \n",
       "0       2.745     11.110     0.482      0.374   0.050     0.000  \n",
       "1       3.829     10.044     0.531      0.168   0.008     0.000  \n",
       "2       4.937      7.988     0.563      0.201   0.029     0.001  \n",
       "3       3.462     10.177     0.588      0.288   0.081     0.000  \n",
       "4       5.089      8.093     0.629      0.248   0.027     0.000  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### If initial pipeline has already run:\n",
    "### Import the dfs from spreadsheets folder \n",
    "\n",
    "df_nnovels_meta = open_df_and_print(file_name = 'df_nnovels_meta.csv', path_to_spreadsheets=path_to_spreadsheets, drop_first_column=True)\n",
    "df_nnovels_meta.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63d66c61-4ce0-4293-b7d4-283b80b35716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "744238.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nnovels_meta['sentences_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048bbcfc-4b6d-46cc-9b90-06a06cbc1985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560cf49e-4f84-4599-a57f-96d8d5a80232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ce2026-b074-48b0-9392-b792359bc692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373ae52b-c49c-40d6-aeed-20aeee888a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a3c46d-ff78-49ba-bc86-afc8e2efe169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## BEGINNING OF TXTLAB ANALYSIS:\n",
    "# Complete Initial Pipeline on txtLab (needs to be run once):\n",
    "# print(\"Starting pipeline...\")\n",
    "# print(\"\\nImporting txtLab corpus from \"+ str(path_to_txtlab)+\"...\")\n",
    "# df_txtlab = pd.read_csv(path_to_txtlab)\n",
    "# df_txtlab.drop(df_txtlab.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# print('\\nCleaning up corpus and grabbing basic statistics...')\n",
    "# df_txtlab = clean_up_corpus_and_grab_basic_stats(df_txtlab, path_to_assets + exclude, path_to_assets + stopwords, dirty_text_column='DIRTY_TEXT')\n",
    "\n",
    "# print('\\nRunning TTR analysis (this can take some time)')\n",
    "# df_txtlab = run_ttr_analysis_on_df(df_txtlab)\n",
    "\n",
    "# print('\\nRunning POS analysis (this can take some time)')\n",
    "# df_txtlab = get_POS_tags_for_text_in_df(df_txtlab, text_row_to_analyze='words_as_string_for_vectorizor')\n",
    "\n",
    "# print('\\nSaving df')\n",
    "# output_metadata(df_txtlab, spreadsheet_name='df_textlab_meta')\n",
    "# output_full(df_txtlab, spreadsheet_name='df_textlab_full')\n",
    "# print('\\nPrinting df')\n",
    "# df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1764f-51e6-42ea-86a5-c0d71e5bd61f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec9f9d9-b7eb-42e6-a183-565399497389",
   "metadata": {},
   "outputs": [],
   "source": [
    "### If initial pipeline has already run:\n",
    "### Import the dfs from spreadsheets folder \n",
    "\n",
    "df_txtlab_full = open_df_and_print(file_name = 'df_txtlab_full.csv', path_to_spreadsheets = path_to_spreadsheets, drop_first_column=True)\n",
    "df_txtlab_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b652f9-84b0-477b-8a95-07fb528107c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_txtlab_full.rename(columns = {'author':'author_name', 'title':'book_title'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b795c1-2dd2-4adb-a301-8b88c71099a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b883739d-d9ea-4024-ab48-a4a04d47fe32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8517388-433d-48e2-80d9-7d62a99b2cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE BOTH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0499c7ee-5887-4132-bb3f-cddc46747b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nSaving NNOVELS df')\n",
    "output_metadata(df_nnovels_full, spreadsheet_name = 'df_nnovels_meta')\n",
    "output_full(df_nnovels_full, spreadsheet_name = 'df_nnovels_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89003e0f-f3d2-42be-813b-bf56669c123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nSaving TXTLAB df')\n",
    "output_metadata(df_txtlab_full, spreadsheet_name = 'df_txtlab_meta')\n",
    "output_full(df_txtlab_full, spreadsheet_name = 'df_txtlab_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b8d130-648f-458a-8675-427a81f11eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity NNOVELS:\n",
    "df_nnovels_full = open_df_and_print(file_name = 'df_nnovels_full.csv', path_to_spreadsheets = path_to_spreadsheets, drop_first_column=True)\n",
    "df_nnovels_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba944f8-0771-4015-b400-96bc601ff070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity TXTLAB:\n",
    "df_txtlab = open_df_and_print(file_name = 'df_textlab_full.csv', path_to_spreadsheets = path_to_spreadsheets, drop_first_column=True)\n",
    "df_txtlab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eed6d4-bcf4-49ca-bc87-c86adba87768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2586d7d1-288a-4bd9-9541-b565e170af00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc61fbc-12c2-4b6a-80f5-437cc291a59d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c5ab8-38b7-4e1b-8a4d-4eb199fa2bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
