{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51087402",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is the main jupyter notebook for The Ciphers of <i>the Times</i>' computational analysis of our curated corpus of 'Newspaper Novels'\n",
    "### We begin by importing the corpus from .txt files into a dataframe, and proceed to clean, extract metadata, and save the new\n",
    "### spreadsheets. \n",
    "###\n",
    "###\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678450ac-4c9b-4ad1-acf8-08698ad629fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful links:\n",
    "# https://towardsdatascience.com/30-very-useful-pandas-functions-for-everyday-data-analysis-tasks-f1eae16409af\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4630a5-d7ce-46ac-a3a9-29e17def8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE PATHS:\n",
    "### Paths are defined for the various spreadsheets and corpora used\n",
    "### Stopwords were initially taken from: https://towardsdatascience.com/getting-started-with-text-analysis-in-python-ca13590eb4f7\n",
    "### and then tailored to our pipeline\n",
    "\n",
    "## to Newpaper Novel corpus:\n",
    "path_to_nnovels_corpus = '../../data/corpora/corpus_newspaper_novels/' \n",
    "\n",
    "## to assets:\n",
    "path_to_assets = '../../assets/'\n",
    "\n",
    "## to all spreadsheets:\n",
    "path_to_spreadsheets = '../../data/spreadsheets/'\n",
    "\n",
    "## to .txtLab corpus consisting of 150 English-language novels:\n",
    "# path_to_dirty_txtlab_corpus = '../../data/spreadsheets/dirtyengnovels-211215.csv' ## This no longer exists, left for pipeline viewing. See df_txtlab_meta.csv for results. \n",
    "# Full DF is too large to upload to GitHub upload.\n",
    "\n",
    "\n",
    "### FILES:\n",
    "\n",
    "## nnovels corpus metadata:\n",
    "nnovels_corpus_metadata = '../../data/spreadsheets/nnovels_corpus_metadata.csv' \n",
    "\n",
    "## stop words:\n",
    "stopwords = 'stopwords.txt'\n",
    "\n",
    "## characters and numbers to exclude from texts:\n",
    "exclude = 'characters_and_numbers_to_exclude.txt'\n",
    "\n",
    "# opening stopwords and characters to exclude from assets\n",
    "with codecs.open(path_to_assets + stopwords, 'r', encoding='utf-8', errors=\"ignore\") as stopwords_raw:\n",
    "    stopwords = stopwords_raw.read()\n",
    "    stopwords = stopwords.split()\n",
    "with codecs.open(path_to_assets + characters_to_exclude, 'r', encoding='utf-8', errors=\"ignore\") as characters_to_exclude_raw:\n",
    "    characters_to_exclude = characters_to_exclude_raw.read()\n",
    "    characters_to_exclude = characters_to_exclude.split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83095e22-84b3-4f8b-a47f-588edea343b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports, utility, and important functions begin here:\n",
    "\n",
    "## basic libraries:\n",
    "import os\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "# lexical diversity library:\n",
    "from lexicalrichness import LexicalRichness\n",
    "\n",
    "# to see more columns when dataframes are printed out:\n",
    "pd.set_option('display.max_columns', 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc2971-ff32-48b9-8fa2-82b01e1b0412",
   "metadata": {},
   "outputs": [],
   "source": [
    "### UTILITY CODE:\n",
    "\n",
    "## This section allows us to open large dataframes by redefining the max size of the csv fields.\n",
    "## Solution taken from: https://stackoverflow.com/questions/15063936/csv-error-field-larger-than-field-limit-131072\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10 as long as the OverflowError occurs\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7194395f-b662-4503-8ef4-5f9c4edd5595",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS 1/6\n",
    "### Various functions for the project begin here \n",
    "\n",
    "## Progress bar to view the progress of lengthy processes\n",
    "# As suggested by Rom Ruben (see: http://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console/27871113#comment50529068_27871113)\n",
    "def progress(count, total, status=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "    percents = round(100.1 * count / float(total), 1)\n",
    "    bar = '#' * filled_len + '-' * (bar_len - filled_len)\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n",
    "    sys.stdout.flush()  \n",
    "    \n",
    "\n",
    "## Used to incorperate a metadata spreadsheet and gather an already ordered corpus within a file:\n",
    "def import_corpus_and_meta(path_to_corpus=path_to_nnovels_corpus, path_to_meta_data=nnovels_corpus_metadata):\n",
    "    # read in metadata\n",
    "    df = pd.read_csv(path_to_meta_data, engine='python')\n",
    "    # drop faulty index\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    # setting up text_index to ensure sequentiallity\n",
    "    text_index = 0\n",
    "    # grab all texts from corpus and strip of project gutenberg end text\n",
    "    #TODO: THIS PROCESS NEEDS TO BE COMPLETED MANUALLY ON CORPUS AND DELETED\n",
    "    split_on = [\"END OF THE PROJECT GUTENBERG\",\"End of the Project Gutenberg EBook\",\"End of Project Gutenberg\",\"End of The Project Gutenberg\"] \n",
    "    for textname in os.listdir(path_to_nnovels_corpus):\n",
    "        with codecs.open(path_to_nnovels_corpus + textname, 'r', encoding='utf-8', errors=\"ignore\") as raw_text:\n",
    "            dirty_text = raw_text.read()\n",
    "            for text in split_on:\n",
    "                dirty_text = dirty_text.split(text)[0]\n",
    "            df.at[text_index, 'dirty_text'] = dirty_text\n",
    "            text_index += 1\n",
    "        progress(text_index, len(os.listdir(path_to_corpus)))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6611189a-65e7-48c6-81c4-e371cf6c91f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS 2/6\n",
    "\n",
    "## Cleans a text string and returns the cleaned text, \n",
    "## the text without stopwords filtered (string), the text with stopwords filtered (list), and the text as sentences (list)\n",
    "def clean_text(text, stopwords=stopwords, characters_to_exclude=characters_to_exclude):\n",
    "    # lowercasing the text\n",
    "    text = text.lower()\n",
    "    # removing all characters except those in split_by\n",
    "    text = ''.join(char for char in text if char not in characters_to_exclude)\n",
    "    # replacing all newline '\\n' with spaces\n",
    "    text = text.replace('\\n', \" \")\n",
    "    # replacing all multiple spaces with a single space\n",
    "    text = re.sub('\\s+',' ', text)\n",
    "    # if sentences is set to true in the function, this will also split text by sentences and input in data frame\n",
    "    # splitting by spaces\n",
    "    text = re.split(r\"\\s\", text)\n",
    "    # removing all empty list items\n",
    "    text_split_stopless = list(filter(None, text))\n",
    "    # getting list of sentences\n",
    "    text_split_sentences = re.split(r\"\\.|\\:|\\?|\\!\", text)\n",
    "    # getting rid of empty elements\n",
    "    text_split_sentences = list(filter(None, text_split_sentences))\n",
    "    # getting rid of end-of-line punctuation:\n",
    "    text_split_stopless = [word.strip(\".?:!\") for word in text_split_stopless]\n",
    "    # getting rid of all stopwords:\n",
    "    text_split_stopped = [word for word in text_split_stopless if word not in stopwords]\n",
    "\n",
    "    return text, text_split_stopless, text_split_stopped, text_split_sentences\n",
    "\n",
    "\n",
    "## Takes a dataframe, a dirty_text column, a characters_to_exclude file, and a stopwords file and returns the clean text, inputting it into the df\n",
    "def clean_up_corpus_and_grab_basic_stats(df, characters_to_exclude, stopwords, dirty_text_column='dirty_text'):\n",
    "    print(\"Cleaning text, assigning them to columns, and grabbing basic stats...\")\n",
    "    # creating columns for collected words:\n",
    "    df['words_standardized_stopped'] = ''\n",
    "    df['sentences_count'] = 0\n",
    "    df['average_words_per_sentence'] = 0.0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        text, text_split_stopless, text_split_stopped, text_split_sentences = clean_text(row[dirty_text_column])\n",
    "\n",
    "        # getting basic stats for tokenized texts (words):\n",
    "        words_count_stopless = len(text_split_stopless)\n",
    "        words_count_stopped = len(text_split_stopped)\n",
    "        percentage_stopped_of_stoppless = (words_count_stopped / words_count_stopless) * 100\n",
    "\n",
    "        # getting basic stats for tokenized texts (sentences):\n",
    "        sentences_count = len(text_split_sentences)\n",
    "        words_per_sentence = [len(sentence.split()) for sentence in text_split_sentences]\n",
    "        total = sum(words_per_sentence)\n",
    "        average_words_per_sentence = int(total) / len(words_per_sentence)\n",
    "        \n",
    "        # inputting data into df\n",
    "        df.at[index, 'words_as_string_for_vectorizor'] = text\n",
    "        df.at[index, 'words_count_stopless'] = words_count_stopless\n",
    "        df.at[index, 'words_count_stopped'] = words_count_stopped\n",
    "        df.at[index, 'words_standardized_stopped'] = text_split_stopped\n",
    "        df.at[index, 'percentage_stopped_of_stoppless'] = percentage_stopped_of_stoppless\n",
    "        df.at[index, 'sentences_standardized_stopless'] = text_split_sentences\n",
    "        df.at[index, 'sentences_count'] = sentences_count\n",
    "        df.at[index, 'average_words_per_sentence'] = average_words_per_sentence\n",
    "\n",
    "        # show progress bar\n",
    "        progress(index, len(df.index))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06f55ce-4001-4798-aa4c-0116837d84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS 3/6\n",
    "\n",
    "## Takes a dataframe, a text column (string), and several parameters, and returns the ttr and mattr (with multiple configurations) for each row:\n",
    "def run_ttr_analysis_on_df(df, text_column='words_as_string_for_vectorizor', full_text_ttr=True, moving_average_ttr=True, window_sizes=[500, 2000]):\n",
    "    for index, row in df.iterrows():\n",
    "        # grabs text column from df\n",
    "        text = row[text_column]\n",
    "        # gets lex from lexical richness library\n",
    "        lex = LexicalRichness(text)\n",
    "        # these switches are here in case someone wants to run just ttr/mattr (since this can take some time)\n",
    "        if full_text_ttr == True:\n",
    "            ttr = lex.ttr\n",
    "            df.loc[index, 'full_text_ttr'] = ttr\n",
    "        if moving_average_ttr == True:\n",
    "            for window_size in window_sizes:\n",
    "                if (window_size != None) and (len(text) > window_size):\n",
    "                    mattr = lex.mattr(window_size=window_size)\n",
    "                    df.loc[index, f'mattr_{str(window_size)}'] = mattr\n",
    "                    \n",
    "        # show progress bar\n",
    "        progress(index, len(df.index))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acfd123-be6b-4388-8a03-7f4abda47a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS 4/6\n",
    "\n",
    "## Takes in a dataframe, a list of columns to visualize, and a date_column, then visualizes it over time using matplot.\n",
    "## Created as a slightly simpler way to check and compare visualizations between different columns and/or dataframes\n",
    "\n",
    "#TODO: make multiple dfs visualizable:\n",
    "def visualize_numerical_columns__over_time(df, list_of_columns_to_visualize, date_column='book_year', graph_y_label='What are we counting?', title='SOMETHING over Time'):\n",
    "    # importing libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "    # setting up lists to capture the years, counts, labels, and colours to visualize\n",
    "    years_list = []\n",
    "    counts_list = []\n",
    "    labels_list = []\n",
    "    colors_list = []\n",
    "    for column_name in list_of_columns_to_visualize:\n",
    "        # grouping the column by years and getting arrays for graph\n",
    "        grouped_by_year = pd.to_numeric(df[column_name]).groupby(df[date_column])\n",
    "        grouped_by_year = grouped_by_year.mean().reset_index()\n",
    "        years = np.array(grouped_by_year[date_column].tolist())\n",
    "        count_to_visualize = np.array(grouped_by_year[column_name].tolist())\n",
    "        # saving the info for each column to visualize in a list\n",
    "        years_list.append(years)\n",
    "        counts_list.append(count_to_visualize)\n",
    "        labels_list.append(column_name)\n",
    "        # getting random colors to differenciate between visualized data\n",
    "        r = random.random()\n",
    "        b = random.random()\n",
    "        g = random.random()\n",
    "        color = (r, g, b)\n",
    "        colors_list.append(color)\n",
    "        \n",
    "    # plotting SOMETHING over time:\n",
    "    plt.figure(figsize=(20,10))\n",
    "    for index in range(len(labels_list)):\n",
    "        plt.plot(years_list[index], counts_list[index], label=labels_list[index], c=colors_list[index])\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7b377-33ad-4eaa-aa00-552c0bae7b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS 5/6\n",
    "## POS tagging:\n",
    "##\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_POS_tags_for_text_in_df(df, text_row_to_analyze='words_as_string_for_vectorizor', \n",
    "                                count_nouns=True, how_many_nouns=20, count_verbs_and_aux=True, how_many_verbs=20):\n",
    "    df['all_pos_counts'] = ''\n",
    "    if count_nouns == True:\n",
    "        df[str(how_many_nouns)+'_most_common_nouns'] = ''\n",
    "    if count_verbs_and_aux == True:\n",
    "        df[str(how_many_verbs)+'_most_common_verbs'] = ''\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        progress(index, len(df.index))\n",
    "        # setting up lists to capture POS tags\n",
    "        nouns = []\n",
    "        verbs_and_aux = []\n",
    "        adjectives = []\n",
    "        text = row[text_row_to_analyze]\n",
    "        # this is a memory buffer, to extend max length of available ram according to the text being analyzed\n",
    "        # https://datascience.stackexchange.com/questions/38745/increasing-spacy-max-nlp-limit\n",
    "        nlp.max_length = len(text) + 100\n",
    "        # disable modules not in use to save memory\n",
    "        analyzed_doc = nlp(text, disable = ['ner'])\n",
    "        \n",
    "        # grabbing all pos counts in the text in non-human readable format\n",
    "        pos_counts_in_text = analyzed_doc.count_by(spacy.attrs.IDS['POS'])\n",
    "        # setting up list to render pos hashes in human readable format:\n",
    "        human_readable_pos_count_list = []\n",
    "        # iterating through counts to make hashes human readable:\n",
    "        for pos, count in pos_counts_in_text.items():\n",
    "            human_readable_tag = analyzed_doc.vocab[pos].text\n",
    "            add_me = list((human_readable_tag, count))\n",
    "            human_readable_pos_count_list.append(add_me)\n",
    "            \n",
    "        for element in human_readable_pos_count_list:\n",
    "            \n",
    "            \n",
    "            # add_me = list((element[1], percentage_of_POS))\n",
    "            df.at[index, 'POS_' + str(element[0])+'_percentage'] = element[1]\n",
    "\n",
    "        \n",
    "        # grabbing all nouns and verbs/auxilaries to find top 20 for each:\n",
    "        for token in analyzed_doc:\n",
    "            if count_nouns == True:\n",
    "                if token.pos_ == 'NOUN':\n",
    "                    nouns.append(token.text)\n",
    "            if count_verbs_and_aux == True:\n",
    "                if token.pos_ == ('VERB' or 'AUX'):\n",
    "                    verbs_and_aux.append(token.text)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # count up all nouns and verbs\n",
    "        nouns_tally = Counter(nouns)\n",
    "        verbs_and_aux_tally = Counter(verbs_and_aux)\n",
    "        \n",
    "        # setting up lists to grab nouns and verbs\n",
    "        total_counts = 0\n",
    "        noun_counts = 0\n",
    "        verb_and_aux_counts = 0\n",
    "        # grab just nouns and verbs counts\n",
    "        for element in human_readable_pos_count_list:\n",
    "            total_counts += element[1]\n",
    "            if element[0] == 'NOUN':\n",
    "                noun_counts = element[1]\n",
    "            if element[0] == ('VERB' or 'AUX'):\n",
    "                verb_and_aux_counts = element[1]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        percentage_nouns_of_pos_counts = round(noun_counts / total_counts * float(100), 3)\n",
    "        percentage_verbs_and_aux_of_pos_counts = round(verb_and_aux_counts / total_counts * float(100), 3)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # setting each in their proper column\n",
    "        \n",
    "\n",
    "        df.at[index, 'noun_percentage'] = percentage_nouns_of_pos_counts\n",
    "        df.at[index, 'verb_and_aux_percentage'] = percentage_verbs_and_aux_of_pos_counts\n",
    "        df.at[index, 'all_pos_counts'] = human_readable_pos_count_list\n",
    "        df.at[index, str(how_many_nouns)+'_most_common_nouns'] = nouns_tally.most_common(how_many_nouns)\n",
    "        df.at[index, str(how_many_verbs)+'_most_common_verbs'] = verbs_and_aux_tally.most_common(how_many_verbs)\n",
    "        progress(index, len(df.index))\n",
    "    return df\n",
    "\n",
    "\n",
    "def separate_pos_tags_into_usable_data(list_or_string_of_POS_counts):\n",
    "    exclude = [\"[\", \"]\", \"'\"]\n",
    "    human_readable_pos_count_list = []\n",
    "    if isinstance(list_or_string_of_POS_counts,str):\n",
    "        print('string!')\n",
    "        # This entire section is an attempt to to save memory, ignore:\n",
    "        pos_counts = list_or_string_of_POS_counts\n",
    "        # get rid of brackets\n",
    "        pos_counts = pos_counts[1:-1]\n",
    "        pos_counts = ''.join(char for char in pos_counts if char not in exclude)\n",
    "        pos_counts_split = pos_counts.split(\",\")   \n",
    "        list_of_POS = []\n",
    "        list_of_values = []\n",
    "        for i in range(len(pos_counts_split)):\n",
    "            if i % 2 == 0:\n",
    "                list_of_POS.append(pos_counts_split[i])\n",
    "            else:\n",
    "                list_of_values.append(pos_counts_split[i])\n",
    "        for i in range(len(list_of_POS)):\n",
    "            add_me = list((list_of_POS[i].strip(), float(list_of_values[i].strip())))\n",
    "            human_readable_pos_count_list.append(add_me)\n",
    "    elif isinstance(list_or_string_of_POS_counts,list):\n",
    "        human_readable_pos_count_list = list_or_string_of_POS_counts \n",
    "    else:\n",
    "        print(\"Function input must be string or list.\")\n",
    "    return human_readable_pos_count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992bf5d2-2284-4c2a-a172-3692797834c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS 6:\n",
    "# Topic Modeling\n",
    "# https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "\n",
    "# Import tools:\n",
    "# pprint\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# Functions:\n",
    "def texts_to_words(texts):\n",
    "    i = 0\n",
    "    for text in texts:\n",
    "        yield(gensim.utils.simple_preprocess(str(texts), deacc=True))  # deacc=True removes punctuations\n",
    "        progress(i, len(texts))\n",
    "        i += 1\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stopwords] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "def topic_model_the_df(df, text_column='words_as_string_for_vectorizor'):\n",
    "    \n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f66625f-c2ae-4945-8967-e1dfae342a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for metadata output:\n",
    "def output_metadata(df, spreadsheet_name = 'BLA', path_to_spreadsheets = path_to_spreadsheets):\n",
    "    list_of_columns_not_to_include = ['words_standardized_stopped', 'sentences_standardized_stopless','words_as_string_for_vectorizor', 'dirty_text']\n",
    "    columns_to_include = [column_name for column_name in df.columns.values.tolist() if column_name.lower() not in list_of_columns_not_to_include]\n",
    "    df_meta = df[columns_to_include]\n",
    "    df_meta.to_csv(path_to_spreadsheets + spreadsheet_name + '.csv')\n",
    "    print(spreadsheet_name + ' was saved in '+str(path_to_spreadsheets))\n",
    "    \n",
    "# for full output:\n",
    "def output_full(df, spreadsheet_name = 'BLA', path_to_spreadsheets = path_to_spreadsheets):\n",
    "    df.to_csv(path_to_spreadsheets + spreadsheet_name + '.csv')\n",
    "    print(spreadsheet_name + ' was saved in '+str(path_to_spreadsheets))\n",
    "    \n",
    "def open_df_and_print(file_name = 'df_full.csv', path_to_spreadsheets = path_to_spreadsheets, drop_first_column=False):\n",
    "    df = pd.read_csv(path_to_spreadsheets + file_name, engine='python')\n",
    "    if drop_first_column == True:\n",
    "        df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d6f8e-b845-4f6f-890c-e2b863ee693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN UP TO HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b452736-e76f-4304-9e64-cdb2638fdbde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a55fb6-c196-400b-b16d-f1b331a62bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfef577-f13c-4fc1-b5e2-b2313832e4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce9fda4-37d4-429c-bb1d-234aa920342a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f3015f-93ca-4d59-bd11-dbb03cb5754c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c70eb3a-7f3c-411a-a4d9-8952a746fdc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d52a60-573d-4733-af3c-8db85d0889c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2add44-412e-4915-88e4-4ad9f70383f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c7b700-9cd4-4a9f-b82a-f9acd9dab2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bdaddb-2b57-4288-9ce5-5b973de90c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f34f759-1194-4883-a930-85c01cd7db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## BEGINNING OF NNOVEL ANALYSIS:\n",
    "# ## IF THIS NEEDS TO BE RUN, UNCOMMENT EVERYTHING AND RUN (make sure that the paths at the top of the notebook correspond with your paths)\n",
    "\n",
    "\n",
    "# # Complete Initial Pipeline (needs to be run once):\n",
    "# print(\"Starting pipeline...\")\n",
    "# print(\"\\nImporting corpus from \"+ str(path_to_nnovels_corpus)+ \" and metadata from \"+ str(path_to_meta_data)+\"...\")\n",
    "# df = import_corpus_and_meta(path_to_nnovels_corpus, path_to_meta_data)\n",
    "# print('\\nCleaning up corpus and grabbing basic statistics...')\n",
    "# df = clean_up_corpus_and_grab_basic_stats(df, path_to_assets + exclude, path_to_assets + stopwords)\n",
    "# df.drop('dirty_text', axis=1, inplace=True)\n",
    "# print('\\nRunning TTR analysis (this can take some time)')\n",
    "# df = run_ttr_analysis_on_df(df)\n",
    "# print('\\nRunning POS analysis (this can take some time)')\n",
    "# df = get_POS_tags_for_text_in_df(df, text_row_to_analyze='words_as_string_for_vectorizor', \n",
    "#                                 count_nouns=True, how_many_nouns=50, count_verbs_and_aux=True, how_many_verbs=50)\n",
    "# print('\\nSaving df')\n",
    "# output_metadata(df, spreadsheet_name = 'df_meta')\n",
    "# output_full(df, spreadsheet_name = 'df_full')\n",
    "# print('\\nPrinting df')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61329ba6-97c6-47d0-999d-3b91b62515fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Getting POS% for each POS tag in texts for NNOVELS:\n",
    "# df_nnovels_full = df_nnovels_full.fillna(0)\n",
    "# for index, row in df_nnovels_full.iterrows():\n",
    "#     total = 0.0\n",
    "#     for name in df_nnovels_full.columns.values.tolist():\n",
    "#         if \"POS_\" in name:\n",
    "#             if not name.startswith(\"%\"):\n",
    "#                 # new_name = name + \"_percentage\"\n",
    "#                 # print(row[name])\n",
    "#                 # print(type(row[name]))\n",
    "#                 total += row[name]\n",
    "#                 # percentage = (int(row[name]) / total) * float(100)\n",
    "#                 # df_txtlab_full.at[index, new_name] = percentage\n",
    "#                 # columns.append(new_name)\n",
    "#                 # df_txtlab_full[new_name] = 0\n",
    "#     df_nnovels_full.at[index, \"parts_of_speech_total_count\"] = int(total)\n",
    "#     for name in df_nnovels_full.columns.values.tolist():\n",
    "#         if \"POS_\" in name:\n",
    "#             if not name.startswith(\"%\"):\n",
    "#                 new_name = \"%\" + name\n",
    "#                 percentage = round((row[name] / total) * float(100), 3)\n",
    "#                 if index == 0:\n",
    "#                     df_nnovels_full[new_name] = 0.0\n",
    "#                 df_nnovels_full.at[index, new_name] = percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11977572-a95f-41b6-8dd1-94b05c90246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ttrs_and_mattrs = ['full_text_ttr', 'mattr_500', 'mattr_2000']\n",
    "# visualize_numerical_columns__over_time(df, ttrs_and_mattrs, date_column='book_year', graph_y_label='TTRs', title='Lexical Diversity in Newspaper Novels over time')\n",
    "\n",
    "# basic_stats_counts = ['words_count_stopless','words_count_stopped','sentences_count','average_words_per_sentence']\n",
    "# visualize_numerical_columns__over_time(df, basic_stats_counts, date_column='book_year', graph_y_label='Counts in Newspaper Novels over time', title='Counts in Newspaper Novels over time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62061b3-55b1-4ad1-a6d1-e10d5cf379e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If initial pipeline has already run:\n",
    "# import exported df to not rerun analysis every time:\n",
    "df_nnovels_full = open_df_and_print(file_name = 'df_nnovels_full.csv', path_to_spreadsheets = path_to_spreadsheets, drop_first_column=True)\n",
    "df_nnovels_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9da087-682a-4b15-b591-dd5fcbdaec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d66c61-4ce0-4293-b7d4-283b80b35716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048bbcfc-4b6d-46cc-9b90-06a06cbc1985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560cf49e-4f84-4599-a57f-96d8d5a80232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ce2026-b074-48b0-9392-b792359bc692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373ae52b-c49c-40d6-aeed-20aeee888a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a3c46d-ff78-49ba-bc86-afc8e2efe169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## BEGINNING OF TXTLAB ANALYSIS:\n",
    "# Complete Initial Pipeline on txtLab (needs to be run once):\n",
    "# print(\"Starting pipeline...\")\n",
    "# print(\"\\nImporting txtLab corpus from \"+ str(path_to_txtlab)+\"...\")\n",
    "# df_txtlab = pd.read_csv(path_to_txtlab)\n",
    "# df_txtlab.drop(df_txtlab.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# print('\\nCleaning up corpus and grabbing basic statistics...')\n",
    "# df_txtlab = clean_up_corpus_and_grab_basic_stats(df_txtlab, path_to_assets + exclude, path_to_assets + stopwords, dirty_text_column='DIRTY_TEXT')\n",
    "\n",
    "# print('\\nRunning TTR analysis (this can take some time)')\n",
    "# df_txtlab = run_ttr_analysis_on_df(df_txtlab)\n",
    "\n",
    "# print('\\nRunning POS analysis (this can take some time)')\n",
    "# df_txtlab = get_POS_tags_for_text_in_df(df_txtlab, text_row_to_analyze='words_as_string_for_vectorizor', \n",
    "#                                 count_nouns=True, how_many_nouns=50, count_verbs_and_aux=True, how_many_verbs=50)\n",
    "\n",
    "# print('\\nSaving df')\n",
    "# output_metadata(df_txtlab, spreadsheet_name = 'df_textlab_meta')\n",
    "# output_full(df_txtlab, spreadsheet_name = 'df_textlab_full')\n",
    "# print('\\nPrinting df')\n",
    "# df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1764f-51e6-42ea-86a5-c0d71e5bd61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Getting POS% for each POS tag in texts for TXTLAB:\n",
    "# ## Second for TXTLAB:\n",
    "# for index, row in df_txtlab_full.iterrows():\n",
    "#     total = 0.0\n",
    "#     for name in df_txtlab_full.columns.values.tolist():\n",
    "#         if \"POS_\" in name:\n",
    "#             if not name.startswith(\"%\"):\n",
    "#                 # new_name = name + \"_percentage\"\n",
    "#                 # print(row[name])\n",
    "#                 # print(type(row[name]))\n",
    "#                 total += row[name]\n",
    "#                 # percentage = (int(row[name]) / total) * float(100)\n",
    "#                 # df_txtlab_full.at[index, new_name] = percentage\n",
    "#                 # columns.append(new_name)\n",
    "#                 # df_txtlab_full[new_name] = 0\n",
    "#     df_txtlab_full.at[index, \"parts_of_speech_total_count\"] = int(total)\n",
    "#     for name in df_txtlab_full.columns.values.tolist():\n",
    "#         if \"POS_\" in name:\n",
    "#             if not name.startswith(\"%\"):\n",
    "#                 new_name = \"%\" + name\n",
    "#                 percentage = round((row[name] / total) * float(100), 3)\n",
    "#                 if index == 0:\n",
    "#                     df_txtlab_full[new_name] = 0.0\n",
    "#                 df_txtlab_full.at[index, new_name] = percentage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ce35cd-e8ea-469c-a067-8d3c16a02ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nSaving df')\n",
    "# output_metadata(df_txtlab, spreadsheet_name = 'df_txtlab_meta')\n",
    "# output_full(df_txtlab, spreadsheet_name = 'df_txtlab_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec9f9d9-b7eb-42e6-a183-565399497389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If initial pipeline has already run:\n",
    "# import exported df to not rerun analysis every time:\n",
    "df_txtlab_full = open_df_and_print(file_name = 'df_txtlab_full.csv', path_to_spreadsheets = path_to_spreadsheets, drop_first_column=True)\n",
    "df_txtlab_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b652f9-84b0-477b-8a95-07fb528107c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_txtlab_full.rename(columns = {'author':'author_name', 'title':'book_title'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8517388-433d-48e2-80d9-7d62a99b2cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE BOTH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0499c7ee-5887-4132-bb3f-cddc46747b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nSaving NNOVELS df')\n",
    "output_metadata(df_nnovels_full, spreadsheet_name = 'df_nnovels_meta')\n",
    "output_full(df_nnovels_full, spreadsheet_name = 'df_nnovels_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89003e0f-f3d2-42be-813b-bf56669c123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nSaving TXTLAB df')\n",
    "output_metadata(df_txtlab_full, spreadsheet_name = 'df_txtlab_meta')\n",
    "output_full(df_txtlab_full, spreadsheet_name = 'df_txtlab_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b8d130-648f-458a-8675-427a81f11eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity NNOVELS:\n",
    "df_nnovels_full = open_df_and_print(file_name = 'df_nnovels_full.csv', path_to_spreadsheets = path_to_spreadsheets, drop_first_column=True)\n",
    "df_nnovels_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba944f8-0771-4015-b400-96bc601ff070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity TXTLAB:\n",
    "df_txtlab = open_df_and_print(file_name = 'df_textlab_full.csv', path_to_spreadsheets = path_to_spreadsheets, drop_first_column=True)\n",
    "df_txtlab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eed6d4-bcf4-49ca-bc87-c86adba87768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2586d7d1-288a-4bd9-9541-b565e170af00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc61fbc-12c2-4b6a-80f5-437cc291a59d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c5ab8-38b7-4e1b-8a4d-4eb199fa2bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
