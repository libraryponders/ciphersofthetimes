{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51087402",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is the main jupyter project notebook for The Ciphers of <i>the Times</i>' computational analysis.\n",
    "### Within, we analyze two corpora. One is our custom curated corpus of \"Newspaper Novels,\" a genre of Sensation Novels from the 19th century\n",
    "### that emphasize their relationship to newspapers through content and style. Many of these novels weren coined \"Newspaper Novels\" by 19th century critics,\n",
    "### others were categorized as such by contemporary academics investigating the phenomenon of Agony Columns in the fiction novels of the Victorian era.\n",
    "### The other corpus we analyze here is from the NOVEL450 dataset, curated and analyzed by .txtLAB, a McGill-based research group: https://txtlab.org/\n",
    "### This corpus serves as a form of control corpus, a crutch to help identify particularities about the \"Newspaper Novel\" corpus. \n",
    "\n",
    "### We have attempted to organize this notebook in a way that would allow to run the same pipeline on both corpora with simplicity, clarity, and reproducability in mind.\n",
    "### But, we are by no means professional programmers, and this project has been an opportunity for the <i>the Times</i>' team members to learn and develop our skills\n",
    "### in Digital Humanities computational analysis. \n",
    "\n",
    "### We begin by importing the corpus from .txt files into a dataframe, and proceed to clean, extract metadata (basic statistics, TTR and MATTR, and POS tagging), \n",
    "### and save the new spreadsheets to perpare them for visualization and continued analysis. \n",
    "\n",
    "### The metadata outputs of this notebook already exists within the data/spreadsheets folder on github.\n",
    "### The full dataframes (with texts) can be reproduced by following the instructions below. We unfortunately could not include the dataframes containing\n",
    "### the full texts due to the space restriction on github for a single file.\n",
    "\n",
    "\n",
    "\n",
    "### jupyter lab --notebook-dir=E:/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b4630a5-d7ce-46ac-a3a9-29e17def8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE PATHS:\n",
    "### Paths are defined for the various spreadsheets and corpora used\n",
    "### Stopwords were initially taken from: https://towardsdatascience.com/getting-started-with-text-analysis-in-python-ca13590eb4f7\n",
    "### and then tailored to our pipeline\n",
    "\n",
    "## to Newpaper Novel corpus:\n",
    "path_to_nnovels_corpus = '../../data/corpora/corpus_newspaper_novels/' \n",
    "\n",
    "## to assets:\n",
    "path_to_assets = '../../assets/'\n",
    "\n",
    "## to all spreadsheets:\n",
    "path_to_spreadsheets = '../../data/spreadsheets/'\n",
    "\n",
    "## to .txtLab corpus consisting of 150 English-language novels:\n",
    "# path_to_dirty_txtlab_corpus = '../../data/spreadsheets/dirtyengnovels-211215.csv' ## This no longer exists, left for pipeline viewing. See df_txtlab_meta.csv for results. \n",
    "\n",
    "## nnovels corpus metadata:\n",
    "nnovels_corpus_metadata = '../../data/spreadsheets/nnovels_corpus_metadata.csv' \n",
    "\n",
    "## stop words:\n",
    "stopwords_file = 'stopwords.txt'\n",
    "\n",
    "## characters and numbers to exclude from texts:\n",
    "exclude_file = 'characters_and_numbers_to_exclude.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83095e22-84b3-4f8b-a47f-588edea343b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports, utility, and important functions begin here:\n",
    "\n",
    "## basic libraries:\n",
    "import os\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "# lexical diversity library:\n",
    "from lexicalrichness import LexicalRichness\n",
    "\n",
    "# to be able to see more columns when dataframes are printed out:\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# to not get copy warnings when splitting dataframes\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f42bc8-6955-44c2-805b-0e63bed94e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## STOPWORDS and elements to exclude\n",
    "# opening stopwords and characters to exclude from assets\n",
    "with codecs.open(path_to_assets + stopwords_file, 'r', encoding='utf-8', errors=\"ignore\") as stopwords_raw:\n",
    "    stopwords = stopwords_raw.read()\n",
    "    stopwords = stopwords.split()\n",
    "with codecs.open(path_to_assets + exclude_file, 'r', encoding='utf-8', errors=\"ignore\") as characters_to_exclude_raw:\n",
    "    characters_to_exclude = characters_to_exclude_raw.read()\n",
    "    characters_to_exclude = characters_to_exclude.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78bc2971-ff32-48b9-8fa2-82b01e1b0412",
   "metadata": {},
   "outputs": [],
   "source": [
    "### UTILITY CODE:\n",
    "\n",
    "## This section allows us to open large dataframes by redefining the max size of the csv fields.\n",
    "## Solution taken from: https://stackoverflow.com/questions/15063936/csv-error-field-larger-than-field-limit-131072\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "while True:\n",
    "    # decrease the maxInt value by factor 10 as long as the OverflowError occurs\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7194395f-b662-4503-8ef4-5f9c4edd5595",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS 1/6\n",
    "### Various functions for the project begin here \n",
    "\n",
    "## Progress bar to view the progress of lengthy processes\n",
    "# As suggested by Rom Ruben (see: http://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console/27871113#comment50529068_27871113)\n",
    "def progress(count, total, status=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "    percents = round(100.1 * count / float(total), 1)\n",
    "    bar = '#' * filled_len + '-' * (bar_len - filled_len)\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', status))\n",
    "    sys.stdout.flush()  \n",
    "    \n",
    "\n",
    "## Used to incorperate a metadata spreadsheet and gather an already ordered corpus within a file,\n",
    "## and returns a dataframe \n",
    "def import_corpus_and_meta(path_to_corpus=path_to_nnovels_corpus, path_to_meta_data=nnovels_corpus_metadata):\n",
    "    # read in metadata\n",
    "    df = pd.read_csv(path_to_meta_data, engine='python')\n",
    "    \n",
    "    # drop faulty index\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    \n",
    "    # setting up text_index to ensure sequentiallity\n",
    "    text_index = 0\n",
    "    \n",
    "    # grab all texts from corpus and strip of project gutenberg endtext\n",
    "    #TODO: THIS PROCESS NEEDS TO BE COMPLETED MANUALLY ON CORPUS AND DELETED\n",
    "    split_on = [\"END OF THE PROJECT GUTENBERG\",\"End of the Project Gutenberg EBook\",\"End of Project Gutenberg\",\"End of The Project Gutenberg\"] \n",
    "    \n",
    "    # loop through each novel in the directory, open the file\n",
    "    for textname in os.listdir(path_to_nnovels_corpus):\n",
    "        with codecs.open(path_to_nnovels_corpus + textname, 'r', encoding='utf-8', errors=\"ignore\") as raw_text:\n",
    "            dirty_text = raw_text.read()\n",
    "            \n",
    "            # getting rid of the project gutenberg endtext\n",
    "            for text in split_on:\n",
    "                dirty_text = dirty_text.split(text)[0]\n",
    "                \n",
    "            # input into df\n",
    "            df.at[text_index, 'dirty_text'] = dirty_text\n",
    "            text_index += 1\n",
    "        # show progress\n",
    "        progress(text_index, len(os.listdir(path_to_corpus)))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6611189a-65e7-48c6-81c4-e371cf6c91f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS 2/6\n",
    "\n",
    "## Cleans a text string and returns the cleaned text, \n",
    "## the text without stopwords filtered (string), the text with stopwords filtered (list), and the text as sentences (list)\n",
    "def clean_text(text, stopwords=stopwords, characters_to_exclude=characters_to_exclude):\n",
    "    # lowercasing the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # removing all characters in characters_to_exclude\n",
    "    text = ''.join(char for char in text if char not in characters_to_exclude)\n",
    "    \n",
    "    # replacing all newline '\\n' with spaces\n",
    "    text = text.replace('\\n', \" \")\n",
    "    \n",
    "    # replacing all multiple spaces with a single space\n",
    "    text = re.sub('\\s+',' ', text)\n",
    "    \n",
    "    # getting list of sentences\n",
    "    text_split_sentences = re.split(r\"\\.|\\:|\\?|\\!\", text)\n",
    "    \n",
    "    # getting rid of empty elements in sentences\n",
    "    text_split_sentences = list(filter(None, text_split_sentences))\n",
    "    \n",
    "    # splitting text by spaces for tokenization\n",
    "    text_split = re.split(r\"\\s\", text)\n",
    "    \n",
    "    # removing all empty elements in text_split\n",
    "    text_split_stopless = list(filter(None, text_split))\n",
    "    \n",
    "    # getting rid of end-of-line punctuation:\n",
    "    text_split_stopless = [word.strip(\".?:!\") for word in text_split_stopless]\n",
    "    \n",
    "    # getting rid of all stopwords:\n",
    "    text_split_stopped = [word for word in text_split_stopless if word not in stopwords]\n",
    "\n",
    "    return text, text_split_stopless, text_split_stopped, text_split_sentences\n",
    "\n",
    "\n",
    "## Takes a dataframe, a dirty_text column, a characters_to_exclude file, and a stopwords file and returns the clean text, inputting it into the df\n",
    "def clean_up_corpus_and_grab_basic_stats(df, dirty_text_column='dirty_text'):\n",
    "    print(\"Cleaning text, assigning them to columns, and grabbing basic stats...\")\n",
    "    \n",
    "    # creating columns for data\n",
    "    df['words_standardized_stopped'] = ''\n",
    "    df['sentences_count'] = 0\n",
    "    df['average_words_per_sentence'] = 0.0\n",
    "    df['sentences_standardized_stopless'] = ''\n",
    "    \n",
    "    # loop through the dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        # get the clean text for each novel\n",
    "        text, text_split_stopless, text_split_stopped, text_split_sentences = clean_text(row[dirty_text_column])\n",
    "\n",
    "        # getting basic stats for tokenized texts (words):\n",
    "        words_count_stopless = len(text_split_stopless)\n",
    "        words_count_stopped = len(text_split_stopped)\n",
    "        percentage_stopped_of_stoppless = (words_count_stopped / words_count_stopless) * 100\n",
    "\n",
    "        # getting basic stats for tokenized texts (sentences):\n",
    "        sentences_count = len(text_split_sentences)\n",
    "        words_per_sentence = [len(sentence.split()) for sentence in text_split_sentences]\n",
    "        total = sum(words_per_sentence)\n",
    "        average_words_per_sentence = int(total) / len(words_per_sentence)\n",
    "        \n",
    "        # inputting data into df\n",
    "        df.at[index, 'words_as_string_for_vectorizor'] = text\n",
    "        df.at[index, 'words_count_stopless'] = words_count_stopless\n",
    "        df.at[index, 'words_count_stopped'] = words_count_stopped\n",
    "        df.at[index, 'words_standardized_stopped'] = text_split_stopped\n",
    "        df.at[index, 'percentage_stopped_of_stoppless'] = percentage_stopped_of_stoppless\n",
    "        df.at[index, 'sentences_standardized_stopless'] = text_split_sentences\n",
    "        df.at[index, 'sentences_count'] = sentences_count\n",
    "        df.at[index, 'average_words_per_sentence'] = average_words_per_sentence\n",
    "\n",
    "        # show progress bar\n",
    "        progress(index, len(df.index))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c06f55ce-4001-4798-aa4c-0116837d84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS 3/6\n",
    "\n",
    "## Takes a dataframe, a text column (string), and several parameters, and returns the ttr and mattr (with multiple configurations) for each row:\n",
    "def run_ttr_analysis_on_df(df, text_column='words_as_string_for_vectorizor', full_text_ttr=True, moving_average_ttr=True, window_sizes=[500, 2000]):\n",
    "    for index, row in df.iterrows():\n",
    "        # grabs text column from df\n",
    "        text = row[text_column]\n",
    "        # gets lex from lexical richness library\n",
    "        lex = LexicalRichness(text)\n",
    "        # these switches are here in case someone wants to run just ttr/mattr (since this can take some time)\n",
    "        if full_text_ttr == True:\n",
    "            ttr = lex.ttr\n",
    "            df.loc[index, 'full_text_ttr'] = ttr\n",
    "        if moving_average_ttr == True:\n",
    "            for window_size in window_sizes:\n",
    "                if (window_size != None) and (len(text) > window_size):\n",
    "                    mattr = lex.mattr(window_size=window_size)\n",
    "                    df.loc[index, f'mattr_{str(window_size)}'] = mattr\n",
    "                    \n",
    "        # show progress bar\n",
    "        progress(index, len(df.index))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1acfd123-be6b-4388-8a03-7f4abda47a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS 4/6\n",
    "\n",
    "## Takes in a dataframe, a list of columns to visualize, and a date_column, then visualizes it over time using the matplotlib.\n",
    "## Created as a slightly simpler way to check and compare visualizations between different columns and/or dataframes\n",
    "\n",
    "#TODO: make multiple dfs visualizable:\n",
    "def visualize_numerical_columns__over_time(df, list_of_columns_to_visualize, date_column='book_year', graph_y_label='What are we counting?', title='SOMETHING over Time'):\n",
    "    # importing libraries\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "    \n",
    "    # setting up lists to capture the years, counts, labels, and colours to visualize\n",
    "    years_list = []\n",
    "    counts_list = []\n",
    "    labels_list = []\n",
    "    colors_list = []\n",
    "    for column_name in list_of_columns_to_visualize:\n",
    "        \n",
    "        # grouping the column by years and getting arrays for graph\n",
    "        grouped_by_year = pd.to_numeric(df[column_name]).groupby(df[date_column])\n",
    "        grouped_by_year = grouped_by_year.mean().reset_index()\n",
    "        years = np.array(grouped_by_year[date_column].tolist())\n",
    "        count_to_visualize = np.array(grouped_by_year[column_name].tolist())\n",
    "        \n",
    "        # saving the info for each column to visualize in a list\n",
    "        years_list.append(years)\n",
    "        counts_list.append(count_to_visualize)\n",
    "        labels_list.append(column_name)\n",
    "        \n",
    "        # getting random colors to differenciate between visualized data\n",
    "        r = random.random()\n",
    "        b = random.random()\n",
    "        g = random.random()\n",
    "        color = (r, g, b)\n",
    "        colors_list.append(color)\n",
    "        \n",
    "    # plotting SOMETHING over time:\n",
    "    plt.figure(figsize=(20,10))\n",
    "    for index in range(len(labels_list)):\n",
    "        plt.plot(years_list[index], counts_list[index], label=labels_list[index], c=colors_list[index])\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ea7b377-33ad-4eaa-aa00-552c0bae7b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS 5/6\n",
    "## POS tagging:\n",
    "\n",
    "# first loading english language support\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "## Takes in a dataframe and clean text column (as string), and returns the df with POS tags for all the texts\n",
    "## Multiple columns are created, one for each POS tag, and one that contains all POS tags (I did this to more easily\n",
    "## be able to grab POS percentages afterward)\n",
    "def get_POS_tags_for_text_in_df(df, text_row_to_analyze='words_as_string_for_vectorizor'):\n",
    "    \n",
    "    # setting up column for pos counts\n",
    "    df['all_pos_counts'] = ''\n",
    "    \n",
    "    # loop through df and get all POS tags:\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        # grab text\n",
    "        text = row[text_row_to_analyze]\n",
    "        \n",
    "        # this is a memory buffer, to extend max length of available ram according to the text being analyzed\n",
    "        # https://datascience.stackexchange.com/questions/38745/increasing-spacy-max-nlp-limit\n",
    "        nlp.max_length = len(text) + 100\n",
    "        \n",
    "        # disable modules not in use to save memory\n",
    "        analyzed_doc = nlp(text, disable = ['ner'])\n",
    "        \n",
    "        # grabbing all pos counts in the text in non-human readable format\n",
    "        pos_counts_in_text = analyzed_doc.count_by(spacy.attrs.IDS['POS'])\n",
    "        \n",
    "        # setting up list to render pos hashes in human readable format:\n",
    "        human_readable_pos_count_list = []\n",
    "        \n",
    "        # iterating through counts to make hashes human readable:\n",
    "        for pos, count in pos_counts_in_text.items():\n",
    "            human_readable_tag = analyzed_doc.vocab[pos].text\n",
    "            # rendering as list to input back into df\n",
    "            human_readable_tag_and_count = list((human_readable_tag, count))\n",
    "            human_readable_pos_count_list.append(human_readable_tag_and_count)\n",
    "        \n",
    "        # looping through the human readable counts, assigning their label to the column\n",
    "        # and the count to the row for each pos tag\n",
    "        for element in human_readable_pos_count_list:\n",
    "            df.at[index, 'POS_' + str(element[0])+'_count'] = element[1]\n",
    "        \n",
    "        # placing all the pos counts for each text in the all_pos_counts column\n",
    "        df.at[index, 'all_pos_counts'] = human_readable_pos_count_list\n",
    "        \n",
    "        # show progress\n",
    "        progress(index, len(df.index))\n",
    "        \n",
    "    # getting POS percentages for each POS tag in texts\n",
    "    # There are much easier and more efficient ways to do this rather than looping over the entire df again but we were pressed for time...\n",
    "    # TODO: integrate this loop into previous loop\n",
    "    for index, row in df.iterrows():\n",
    "        total = 0.0\n",
    "        for name in df.columns.values.tolist():\n",
    "            if name.startswith(\"POS_\"):\n",
    "                # get total POS elements count for sanity\n",
    "                total += row[name]\n",
    "        df.at[index, \"parts_of_speech_total_count\"] = int(total)\n",
    "        \n",
    "        for name in df.columns.values.tolist():\n",
    "            if name.startswith(\"POS_\"):\n",
    "                # assign new name for column\n",
    "                new_name = \"%\" + name\n",
    "                # get % of total POS in text\n",
    "                percentage = round((row[name] / total) * float(100), 3)\n",
    "                # if this is the first index, create the column name to avoid errors\n",
    "                if index == 0:\n",
    "                    df[new_name] = 0.0\n",
    "                df.at[index, new_name] = percentage\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "992bf5d2-2284-4c2a-a172-3692797834c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS 6/6\n",
    "\n",
    "## Functions that 1) save just metadata, 2) save the full dataframe, 3) load the dataframe\n",
    "\n",
    "# for metadata output:\n",
    "def output_metadata(df, spreadsheet_name='FILL_IN_SPREADSHEET_NAME_META', path_to_spreadsheets=path_to_spreadsheets):\n",
    "    # setting up list of columns NOT to export (since this is just metadata)\n",
    "    list_of_columns_not_to_include = ['words_standardized_stopped', 'sentences_standardized_stopless','words_as_string_for_vectorizor', 'dirty_text']\n",
    "    # all other columns are included\n",
    "    columns_to_include = [column_name for column_name in df.columns.values.tolist() if column_name.lower() not in list_of_columns_not_to_include]\n",
    "    df_meta = df[columns_to_include]\n",
    "    df_meta.to_csv(path_to_spreadsheets + spreadsheet_name + '.csv')\n",
    "    print(spreadsheet_name + ' was saved in '+str(path_to_spreadsheets))\n",
    "    \n",
    "# for full output:\n",
    "def output_full(df, spreadsheet_name='FILL_IN_SPREADSHEET_NAME_FULL', path_to_spreadsheets=path_to_spreadsheets):\n",
    "    df.to_csv(path_to_spreadsheets + spreadsheet_name + '.csv')\n",
    "    print(spreadsheet_name + ' was saved in '+str(path_to_spreadsheets))\n",
    "\n",
    "# load a dataframe\n",
    "def open_df_and_print(file_name='df_full.csv', path_to_spreadsheets=path_to_spreadsheets, drop_first_column=False):\n",
    "    df = pd.read_csv(path_to_spreadsheets + file_name, engine='python')\n",
    "    if drop_first_column == True:\n",
    "        df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    return df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f66625f-c2ae-4945-8967-e1dfae342a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c2d6f8e-b845-4f6f-890c-e2b863ee693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN UP TO HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b452736-e76f-4304-9e64-cdb2638fdbde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a55fb6-c196-400b-b16d-f1b331a62bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2add44-412e-4915-88e4-4ad9f70383f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba24ada6-9897-4ab1-aafd-a45a4ad85676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503f9c51-3592-4f1e-89a8-1af8479051bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f34f759-1194-4883-a930-85c01cd7db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## BEGINNING OF NNOVEL ANALYSIS:\n",
    "# # ## IF THIS NEEDS TO BE RUN, UNCOMMENT EVERYTHING AND RUN (make sure that the paths at the top of the notebook correspond with your paths)\n",
    "\n",
    "# # Complete Initial Pipeline (needs to be run once):\n",
    "# print(\"Starting pipeline...\")\n",
    "\n",
    "# print(\"\\nImporting corpus from \"+ str(path_to_nnovels_corpus)+ \" and metadata from \"+ str(nnovels_corpus_metadata))\n",
    "# df = import_corpus_and_meta(path_to_nnovels_corpus, nnovels_corpus_metadata)\n",
    "\n",
    "# # for testing purposes: \n",
    "# # df = df_all.loc[:5,:]\n",
    "\n",
    "# print('\\nCleaning up corpus and grabbing basic statistics')\n",
    "# df = clean_up_corpus_and_grab_basic_stats(df)\n",
    "# df.drop('dirty_text', axis=1, inplace=True)\n",
    "\n",
    "# print('\\nRunning TTR analysis (this can take some time)')\n",
    "# df = run_ttr_analysis_on_df(df)\n",
    "\n",
    "# print('\\nRunning POS analysis (this can take some time)')\n",
    "# df = get_POS_tags_for_text_in_df(df, text_row_to_analyze='words_as_string_for_vectorizor')\n",
    "\n",
    "# df.head()\n",
    "# print('\\nSaving df')\n",
    "# output_metadata(df, spreadsheet_name = 'df_meta')\n",
    "# output_full(df, spreadsheet_name = 'df_full')\n",
    "# print('\\nPrinting df')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc40629c-79db-4a1d-81aa-eb013d9916ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62061b3-55b1-4ad1-a6d1-e10d5cf379e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### If initial pipeline has already run:\n",
    "### Import the dfs from spreadsheets folder \n",
    "\n",
    "df_nnovels_full = open_df_and_print(file_name = 'df_nnovels_full.csv', path_to_spreadsheets=path_to_spreadsheets, drop_first_column=True)\n",
    "df_nnovels_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9da087-682a-4b15-b591-dd5fcbdaec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d66c61-4ce0-4293-b7d4-283b80b35716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048bbcfc-4b6d-46cc-9b90-06a06cbc1985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560cf49e-4f84-4599-a57f-96d8d5a80232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ce2026-b074-48b0-9392-b792359bc692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373ae52b-c49c-40d6-aeed-20aeee888a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a3c46d-ff78-49ba-bc86-afc8e2efe169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## BEGINNING OF TXTLAB ANALYSIS:\n",
    "# Complete Initial Pipeline on txtLab (needs to be run once):\n",
    "# print(\"Starting pipeline...\")\n",
    "# print(\"\\nImporting txtLab corpus from \"+ str(path_to_txtlab)+\"...\")\n",
    "# df_txtlab = pd.read_csv(path_to_txtlab)\n",
    "# df_txtlab.drop(df_txtlab.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# print('\\nCleaning up corpus and grabbing basic statistics...')\n",
    "# df_txtlab = clean_up_corpus_and_grab_basic_stats(df_txtlab, path_to_assets + exclude, path_to_assets + stopwords, dirty_text_column='DIRTY_TEXT')\n",
    "\n",
    "# print('\\nRunning TTR analysis (this can take some time)')\n",
    "# df_txtlab = run_ttr_analysis_on_df(df_txtlab)\n",
    "\n",
    "# print('\\nRunning POS analysis (this can take some time)')\n",
    "# df_txtlab = get_POS_tags_for_text_in_df(df_txtlab, text_row_to_analyze='words_as_string_for_vectorizor')\n",
    "\n",
    "# print('\\nSaving df')\n",
    "# output_metadata(df_txtlab, spreadsheet_name='df_textlab_meta')\n",
    "# output_full(df_txtlab, spreadsheet_name='df_textlab_full')\n",
    "# print('\\nPrinting df')\n",
    "# df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1764f-51e6-42ea-86a5-c0d71e5bd61f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec9f9d9-b7eb-42e6-a183-565399497389",
   "metadata": {},
   "outputs": [],
   "source": [
    "### If initial pipeline has already run:\n",
    "### Import the dfs from spreadsheets folder \n",
    "\n",
    "df_txtlab_full = open_df_and_print(file_name = 'df_txtlab_full.csv', path_to_spreadsheets = path_to_spreadsheets, drop_first_column=True)\n",
    "df_txtlab_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b652f9-84b0-477b-8a95-07fb528107c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_txtlab_full.rename(columns = {'author':'author_name', 'title':'book_title'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b795c1-2dd2-4adb-a301-8b88c71099a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b883739d-d9ea-4024-ab48-a4a04d47fe32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8517388-433d-48e2-80d9-7d62a99b2cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE BOTH:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0499c7ee-5887-4132-bb3f-cddc46747b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nSaving NNOVELS df')\n",
    "output_metadata(df_nnovels_full, spreadsheet_name = 'df_nnovels_meta')\n",
    "output_full(df_nnovels_full, spreadsheet_name = 'df_nnovels_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89003e0f-f3d2-42be-813b-bf56669c123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nSaving TXTLAB df')\n",
    "output_metadata(df_txtlab_full, spreadsheet_name = 'df_txtlab_meta')\n",
    "output_full(df_txtlab_full, spreadsheet_name = 'df_txtlab_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b8d130-648f-458a-8675-427a81f11eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity NNOVELS:\n",
    "df_nnovels_full = open_df_and_print(file_name = 'df_nnovels_full.csv', path_to_spreadsheets = path_to_spreadsheets, drop_first_column=True)\n",
    "df_nnovels_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba944f8-0771-4015-b400-96bc601ff070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity TXTLAB:\n",
    "df_txtlab = open_df_and_print(file_name = 'df_textlab_full.csv', path_to_spreadsheets = path_to_spreadsheets, drop_first_column=True)\n",
    "df_txtlab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eed6d4-bcf4-49ca-bc87-c86adba87768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2586d7d1-288a-4bd9-9541-b565e170af00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc61fbc-12c2-4b6a-80f5-437cc291a59d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c5ab8-38b7-4e1b-8a4d-4eb199fa2bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
